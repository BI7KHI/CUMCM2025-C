#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T2 v2.0ï¼šåŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„ä¼˜åŒ–
ç›®æ ‡ï¼šæœ€å°åŒ–å­•å¦‡æ½œåœ¨é£é™©ï¼Œä¼˜åŒ–NIPTæ—¶ç‚¹é€‰æ‹©

æ–°å¢ç®—æ³•ï¼š
1. ç”Ÿå­˜åˆ†æåˆ†ç»„ (Survival-based Grouping)
2. é£é™©æœ€å°åŒ–ä¼˜åŒ– (Risk Minimization)
3. Coxæ¯”ä¾‹é£é™©æ¨¡å‹
4. Kaplan-Meierç”Ÿå­˜æ›²çº¿åˆ†ç»„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
import os
import random
import matplotlib.font_manager as fm
import warnings
warnings.filterwarnings('ignore')

# å¥å£®çš„ä¸­æ–‡å­—ä½“é…ç½®
def configure_chinese_font():
    try:
        fonts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'fonts')
        fonts_dir = os.path.abspath(fonts_dir)
        os.makedirs(fonts_dir, exist_ok=True)
        
        # åŠ è½½æœ¬åœ°å­—ä½“
        for file_name in os.listdir(fonts_dir):
            if file_name.lower().endswith(('.ttf', '.otf')):
                try:
                    fm.fontManager.addfont(os.path.join(fonts_dir, file_name))
                except Exception:
                    pass
        
        # é‡æ–°åŠ è½½å­—ä½“ç¼“å­˜
        try:
            fm._load_fontmanager(try_read_cache=False)
        except Exception:
            pass
        
        candidate_families = [
            'Noto Sans CJK SC', 'Noto Sans SC', 'Source Han Sans SC',
            'WenQuanYi Zen Hei', 'Microsoft YaHei', 'PingFang SC', 'Arial Unicode MS', 'DejaVu Sans'
        ]
        
        installed = set(f.name for f in fm.fontManager.ttflist)
        for family in candidate_families:
            if family in installed:
                plt.rcParams['font.family'] = ['sans-serif']
                plt.rcParams['font.sans-serif'] = [family, 'DejaVu Sans']
                return family
                
        # å¦‚æœä»æœªæ‰¾åˆ°ï¼Œå°è¯•ä¸‹è½½ NotoSansCJKsc-Regular.otf
        target_path = os.path.join(fonts_dir, 'NotoSansCJKsc-Regular.otf')
        if not os.path.exists(target_path):
            url = 'https://raw.githubusercontent.com/googlefonts/noto-cjk/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf'
            try:
                import urllib.request
                urllib.request.urlretrieve(url, target_path)
                fm.fontManager.addfont(target_path)
                fm._load_fontmanager(try_read_cache=False)
            except Exception:
                pass
        
        installed = set(f.name for f in fm.fontManager.ttflist)
        if 'Noto Sans CJK SC' in installed:
            plt.rcParams['font.family'] = ['sans-serif']
            plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC', 'DejaVu Sans']
            return 'Noto Sans CJK SC'
    except Exception:
        pass
    return None

family = configure_chinese_font()
if family:
    print(f"æˆåŠŸé…ç½®ä¸­æ–‡å­—ä½“: {family}")
else:
    plt.rcParams['font.family'] = ['sans-serif']
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ DejaVu Sans")

plt.rcParams['axes.unicode_minus'] = False

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)

# åˆ›å»ºç»“æœç›®å½•ï¼ˆv2.0 ä¸“ç”¨ï¼‰
results_dir = os.path.join(project_root, 'results_v2.0')
os.makedirs(results_dir, exist_ok=True)

print("=== T2 v2.0ï¼šåŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„ä¼˜åŒ– ===\n")

# è¯»å–æ•°æ®
data_path = os.path.join(project_root, 'Source_DATA', 'dataA.csv')
data = pd.read_csv(data_path, header=None)

# æ ¹æ®é™„å½•1ï¼Œç¡®å®šå„åˆ—çš„ç´¢å¼•
columns = ['æ ·æœ¬åºå·', 'å­•å¦‡ä»£ç ', 'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'æœ«æ¬¡æœˆç»æ—¶é—´',
           'IVFå¦Šå¨ æ–¹å¼', 'æ£€æµ‹æ—¶é—´', 'æ£€æµ‹æŠ½è¡€æ¬¡æ•°', 'å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨', 'å­•å¦‡BMIæŒ‡æ ‡',
           'åŸå§‹æµ‹åºæ•°æ®çš„æ€»è¯»æ®µæ•°', 'æ€»è¯»æ®µæ•°ä¸­åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹', 'æ€»è¯»æ®µæ•°ä¸­é‡å¤è¯»æ®µçš„æ¯”ä¾‹',
           'æ€»è¯»æ®µæ•°ä¸­å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°', 'GCå«é‡', '13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼',
           '21å·æŸ“è‰²ä½“çš„Zå€¼', 'XæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“æµ“åº¦',
           'XæŸ“è‰²ä½“æµ“åº¦', '13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡',
           'è¢«è¿‡æ»¤æ‰çš„è¯»æ®µæ•°å æ€»è¯»æ®µæ•°çš„æ¯”ä¾‹', 'æ£€æµ‹å‡ºçš„æŸ“è‰²ä½“å¼‚å¸¸', 'å­•å¦‡çš„æ€€å­•æ¬¡æ•°',
           'å­•å¦‡çš„ç”Ÿäº§æ¬¡æ•°', 'èƒå„¿æ˜¯å¦å¥åº·']
data.columns = columns

# æ•°æ®é¢„å¤„ç†ï¼šåªä¿ç•™ç”·èƒæ•°æ®
male_fetus_data = data[data['YæŸ“è‰²ä½“æµ“åº¦'].notna()].copy()

# è½¬æ¢æ•°å€¼å‹åˆ—çš„æ•°æ®ç±»å‹
def safe_float_convert(x):
    try:
        return float(x)
    except:
        return np.nan

numeric_columns = ['å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡',
                   'YæŸ“è‰²ä½“æµ“åº¦', 'YæŸ“è‰²ä½“çš„Zå€¼', 'GCå«é‡']

for col in numeric_columns:
    male_fetus_data[col] = male_fetus_data[col].apply(safe_float_convert)

# å°†å­•å‘¨è½¬æ¢ä¸ºæ•°å€¼
def convert_gestational_age(age_str):
    try:
        if isinstance(age_str, str):
            if '+' in age_str:
                weeks, days = age_str.split('w+')
                return float(weeks) + float(days)/7
            elif 'w' in age_str:
                return float(age_str.split('w')[0])
        return float(age_str) if age_str else np.nan
    except:
        return np.nan

male_fetus_data['å­•å‘¨æ•°å€¼'] = male_fetus_data['å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨'].apply(convert_gestational_age)

# æ¸…ç†æ•°æ®
male_fetus_data = male_fetus_data.dropna(subset=['å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼', 'YæŸ“è‰²ä½“æµ“åº¦'])

# === 1. é£é™©å‡½æ•°å®šä¹‰ ===

def calculate_risk_score(bmi, gestational_age, y_concentration, detection_error=0.05):
    """
    è®¡ç®—å­•å¦‡æ½œåœ¨é£é™©è¯„åˆ†
    
    é£é™©å› å­ï¼š
    1. BMIæå€¼é£é™©
    2. å­•å‘¨æ—¶ç‚¹ä¸å½“é£é™©  
    3. YæŸ“è‰²ä½“æµ“åº¦ä¸è¶³é£é™©
    4. æ£€æµ‹è¯¯å·®é£é™©
    """
    risk_score = 0.0
    
    # BMIé£é™©ï¼ˆUå‹æ›²çº¿ï¼‰
    optimal_bmi = 25.0  # ç†æƒ³BMI
    bmi_risk = (bmi - optimal_bmi) ** 2 / 100
    risk_score += bmi_risk
    
    # å­•å‘¨é£é™©ï¼ˆè¿‡æ—©æˆ–è¿‡æ™šæ£€æµ‹ï¼‰
    optimal_week = 13.0  # ç†æƒ³æ£€æµ‹å­•å‘¨
    week_risk = abs(gestational_age - optimal_week) * 0.1
    risk_score += week_risk
    
    # YæŸ“è‰²ä½“æµ“åº¦é£é™©ï¼ˆæµ“åº¦ä¸è¶³å¯¼è‡´æ£€æµ‹å¤±è´¥ï¼‰
    concentration_threshold = 0.3  # å‡è®¾é˜ˆå€¼
    if y_concentration < concentration_threshold:
        conc_risk = (concentration_threshold - y_concentration) * 2
        risk_score += conc_risk
    
    # æ£€æµ‹è¯¯å·®é£é™©
    error_risk = detection_error * bmi * 0.01
    risk_score += error_risk
    
    return risk_score

# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é£é™©è¯„åˆ†
male_fetus_data['é£é™©è¯„åˆ†'] = male_fetus_data.apply(
    lambda row: calculate_risk_score(
        row['å­•å¦‡BMIæŒ‡æ ‡'], 
        row['å­•å‘¨æ•°å€¼'], 
        row['YæŸ“è‰²ä½“æµ“åº¦']
    ), axis=1
)

# === 2. ç”Ÿå­˜åˆ†æç›¸å…³å˜é‡æ„é€  ===

# æ„é€ "äº‹ä»¶"ï¼šYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡ï¼ˆ>é˜ˆå€¼ï¼‰
concentration_threshold = np.percentile(male_fetus_data['YæŸ“è‰²ä½“æµ“åº¦'], 25)  # 25åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
male_fetus_data['äº‹ä»¶_è¾¾æ ‡'] = (male_fetus_data['YæŸ“è‰²ä½“æµ“åº¦'] >= concentration_threshold).astype(int)

# æ„é€ "æ—¶é—´"ï¼šå­•å‘¨æ•°å€¼
male_fetus_data['æ—¶é—´_å­•å‘¨'] = male_fetus_data['å­•å‘¨æ•°å€¼']

# === 3. ç”Ÿå­˜åˆ†æåˆ†ç»„ç®—æ³• ===

def survival_based_grouping(data, n_groups=3):
    """
    åŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„
    ä½¿ç”¨Kaplan-Meierä¼°è®¡å™¨æ‰¾åˆ°æœ€ä¼˜åˆ†ç»„ç‚¹
    """
    print(f"\n=== ç”Ÿå­˜åˆ†æåˆ†ç»„ç®—æ³• (K={n_groups}) ===")
    
    bmi_values = data['å­•å¦‡BMIæŒ‡æ ‡'].values
    bmi_min, bmi_max = bmi_values.min(), bmi_values.max()
    
    # åˆå§‹åˆ†ç»„ç‚¹ï¼ˆç­‰è·ï¼‰
    initial_cuts = np.linspace(bmi_min, bmi_max, n_groups + 1)[1:-1]
    
    best_cuts = initial_cuts.copy()
    best_score = -np.inf
    
    # ä¼˜åŒ–åˆ†ç»„ç‚¹
    for iteration in range(100):
        # æ ¹æ®å½“å‰åˆ†ç»„ç‚¹åˆ’åˆ†æ•°æ®
        labels = np.digitize(bmi_values, bins=best_cuts)
        
        # è®¡ç®—å„ç»„çš„ç”Ÿå­˜å‡½æ•°å·®å¼‚
        survival_score = 0.0
        groups = []
        
        for group_id in np.unique(labels):
            group_data = data[labels == group_id]
            if len(group_data) >= 10:  # ç¡®ä¿ç»„å†…æœ‰è¶³å¤Ÿæ ·æœ¬
                groups.append((group_id, group_data))
        
        # ä½¿ç”¨logrankæ£€éªŒè¯„ä¼°ç»„é—´å·®å¼‚
        if len(groups) >= 2:
            try:
                # ä¸¤ä¸¤æ¯”è¾ƒå„ç»„çš„ç”Ÿå­˜æ›²çº¿
                pairwise_scores = []
                for i in range(len(groups)):
                    for j in range(i+1, len(groups)):
                        group1_data = groups[i][1]
                        group2_data = groups[j][1]
                        
                        # logrankæ£€éªŒ
                        result = logrank_test(
                            group1_data['æ—¶é—´_å­•å‘¨'], group1_data['äº‹ä»¶_è¾¾æ ‡'],
                            group2_data['æ—¶é—´_å­•å‘¨'], group2_data['äº‹ä»¶_è¾¾æ ‡']
                        )
                        # ä½¿ç”¨-log(på€¼)ä½œä¸ºè¯„åˆ†ï¼Œpå€¼è¶Šå°å·®å¼‚è¶Šæ˜¾è‘—
                        score = -np.log(result.p_value + 1e-10)
                        pairwise_scores.append(score)
                
                survival_score = np.mean(pairwise_scores) if pairwise_scores else 0
                
            except Exception as e:
                survival_score = 0
        
        # å¦‚æœæ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„
        if survival_score > best_score:
            best_score = survival_score
            # å¾®è°ƒåˆ†ç»„ç‚¹
            step = (bmi_max - bmi_min) * 0.01
            for i in range(len(best_cuts)):
                best_cuts[i] += np.random.uniform(-step, step)
                best_cuts[i] = np.clip(best_cuts[i], bmi_min, bmi_max)
            best_cuts = np.sort(best_cuts)
        
        if iteration % 20 == 0:
            print(f"  è¿­ä»£ {iteration}: ç”Ÿå­˜åˆ†æè¯„åˆ† = {survival_score:.4f}")
    
    # ç”Ÿæˆæœ€ç»ˆæ ‡ç­¾
    final_labels = np.digitize(bmi_values, bins=best_cuts)
    
    print(f"  ç”Ÿå­˜åˆ†æåˆ†ç»„å®Œæˆï¼Œæœ€ç»ˆè¯„åˆ†: {best_score:.4f}")
    return best_score, best_cuts, final_labels

# === 4. Coxæ¯”ä¾‹é£é™©æ¨¡å‹åˆ†ç»„ ===

def cox_based_grouping(data, n_groups=3):
    """
    åŸºäºCoxæ¯”ä¾‹é£é™©æ¨¡å‹çš„åˆ†ç»„
    """
    print(f"\n=== Coxæ¯”ä¾‹é£é™©æ¨¡å‹åˆ†ç»„ (K={n_groups}) ===")
    
    try:
        # å‡†å¤‡Coxæ¨¡å‹æ•°æ®
        cox_data = data[['å­•å¦‡BMIæŒ‡æ ‡', 'å­•å¦‡å¹´é¾„', 'æ—¶é—´_å­•å‘¨', 'äº‹ä»¶_è¾¾æ ‡', 'é£é™©è¯„åˆ†']].copy()
        cox_data = cox_data.dropna()
        
        # æ‹ŸåˆCoxæ¨¡å‹
        cph = CoxPHFitter()
        cph.fit(cox_data, duration_col='æ—¶é—´_å­•å‘¨', event_col='äº‹ä»¶_è¾¾æ ‡')
        
        # è®¡ç®—é£é™©æ¯”
        risk_scores = cph.predict_partial_hazard(cox_data)
        
        # åŸºäºé£é™©æ¯”è¿›è¡Œåˆ†ç»„
        risk_percentiles = np.percentile(risk_scores, np.linspace(0, 100, n_groups + 1))
        risk_cuts = risk_percentiles[1:-1]
        
        # æ˜ å°„å›BMIç©ºé—´
        bmi_values = cox_data['å­•å¦‡BMIæŒ‡æ ‡'].values
        cox_labels = np.digitize(risk_scores, bins=risk_cuts)
        
        # è®¡ç®—å„ç»„çš„BMIèŒƒå›´
        bmi_cuts = []
        for group_id in np.unique(cox_labels):
            group_bmi = bmi_values[cox_labels == group_id]
            if len(group_bmi) > 0:
                bmi_cuts.append(np.mean([group_bmi.min(), group_bmi.max()]))
        
        bmi_cuts = sorted(bmi_cuts)[:-1]  # å»æ‰æœ€åä¸€ä¸ª
        
        # é‡æ–°ç”ŸæˆåŸºäºBMIçš„æ ‡ç­¾
        final_labels = np.digitize(data['å­•å¦‡BMIæŒ‡æ ‡'].values, bins=bmi_cuts)
        
        # è¯„åˆ†ï¼šä½¿ç”¨æ¨¡å‹çš„concordance index
        cox_score = cph.concordance_index_
        
        print(f"  Coxæ¨¡å‹åˆ†ç»„å®Œæˆï¼ŒC-index: {cox_score:.4f}")
        return cox_score, bmi_cuts, final_labels
        
    except Exception as e:
        print(f"  Coxæ¨¡å‹åˆ†ç»„å¤±è´¥: {e}")
        # å›é€€åˆ°é£é™©è¯„åˆ†åˆ†ç»„
        risk_values = data['é£é™©è¯„åˆ†'].values
        risk_cuts = np.percentile(risk_values, np.linspace(0, 100, n_groups + 1))[1:-1]
        labels = np.digitize(risk_values, bins=risk_cuts)
        return 0.5, risk_cuts, labels

# === 5. é£é™©æœ€å°åŒ–åˆ†ç»„ç®—æ³• ===

def risk_minimization_grouping(data, n_groups=3):
    """
    ç›´æ¥æœ€å°åŒ–æ€»é£é™©çš„åˆ†ç»„ç®—æ³•
    """
    print(f"\n=== é£é™©æœ€å°åŒ–åˆ†ç»„ç®—æ³• (K={n_groups}) ===")
    
    bmi_values = data['å­•å¦‡BMIæŒ‡æ ‡'].values
    risk_values = data['é£é™©è¯„åˆ†'].values
    
    best_cuts = None
    best_total_risk = np.inf
    
    # å¤šæ¬¡éšæœºåˆå§‹åŒ–å¯»æ‰¾æœ€ä¼˜è§£
    for trial in range(50):
        # éšæœºåˆå§‹åŒ–åˆ†ç»„ç‚¹
        cuts = np.sort(np.random.uniform(bmi_values.min(), bmi_values.max(), n_groups - 1))
        
        # æ¨¡æ‹Ÿé€€ç«ä¼˜åŒ–
        current_cuts = cuts.copy()
        current_risk = np.inf
        temperature = 1.0
        
        for iteration in range(200):
            # è®¡ç®—å½“å‰åˆ†ç»„çš„æ€»é£é™©
            labels = np.digitize(bmi_values, bins=current_cuts)
            total_risk = 0.0
            
            for group_id in np.unique(labels):
                group_indices = labels == group_id
                if np.sum(group_indices) > 0:
                    group_risk = np.mean(risk_values[group_indices])
                    group_size = np.sum(group_indices)
                    # åŠ æƒé£é™©ï¼ˆå¤§ç»„çš„æƒé‡æ›´é«˜ï¼‰
                    total_risk += group_risk * group_size
            
            # æ¥å—æ¡ä»¶
            if total_risk < current_risk or np.random.rand() < np.exp(-(total_risk - current_risk) / temperature):
                current_risk = total_risk
                
                if total_risk < best_total_risk:
                    best_total_risk = total_risk
                    best_cuts = current_cuts.copy()
            
            # æ‰°åŠ¨åˆ†ç»„ç‚¹
            if iteration < 199:  # ä¸åœ¨æœ€åä¸€æ¬¡è¿­ä»£æ—¶æ‰°åŠ¨
                idx = np.random.randint(len(current_cuts))
                step = (bmi_values.max() - bmi_values.min()) * 0.02
                current_cuts[idx] += np.random.uniform(-step, step)
                current_cuts[idx] = np.clip(current_cuts[idx], bmi_values.min(), bmi_values.max())
                current_cuts = np.sort(current_cuts)
            
            temperature *= 0.995
        
        if trial % 10 == 0:
            print(f"  è¯•éªŒ {trial}: å½“å‰æœ€ä¼˜æ€»é£é™© = {best_total_risk:.2f}")
    
    # ç”Ÿæˆæœ€ç»ˆæ ‡ç­¾
    final_labels = np.digitize(bmi_values, bins=best_cuts)
    
    # è½¬æ¢ä¸ºæœ€å°åŒ–è¯„åˆ†ï¼ˆè´Ÿé£é™©ï¼‰
    risk_min_score = -best_total_risk / len(data)
    
    print(f"  é£é™©æœ€å°åŒ–åˆ†ç»„å®Œæˆï¼Œå¹³å‡é£é™©: {-risk_min_score:.4f}")
    return risk_min_score, best_cuts, final_labels

# === 6. æ‰§è¡Œæ‰€æœ‰åˆ†ç»„ç®—æ³• ===

print("\n=== å¼€å§‹ç”Ÿå­˜åˆ†æå’Œé£é™©ä¼˜åŒ–åˆ†ç»„ ===")

# ä¼ ç»Ÿç®—æ³•ï¼ˆä½œä¸ºå¯¹æ¯”åŸºå‡†ï¼‰
bmi_values = male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].values.reshape(-1, 1)

# KMeansåŸºå‡†
silhouette_scores_kmeans = []
for k in range(2, 8):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(bmi_values)
    score = silhouette_score(bmi_values, labels)
    silhouette_scores_kmeans.append((k, score))

best_k_kmeans = max(silhouette_scores_kmeans, key=lambda x: x[1])[0]

# æ–°ç®—æ³•æµ‹è¯•
algorithms_results = []

for k in range(3, 6):  # æµ‹è¯•3-5ç»„
    print(f"\n--- æµ‹è¯• {k} ç»„åˆ†ç»„ ---")
    
    # ç”Ÿå­˜åˆ†æåˆ†ç»„
    surv_score, surv_cuts, surv_labels = survival_based_grouping(male_fetus_data, k)
    algorithms_results.append((k, 'Survival', surv_score, surv_cuts, surv_labels))
    
    # Coxæ¨¡å‹åˆ†ç»„
    cox_score, cox_cuts, cox_labels = cox_based_grouping(male_fetus_data, k)
    algorithms_results.append((k, 'Cox', cox_score, cox_cuts, cox_labels))
    
    # é£é™©æœ€å°åŒ–åˆ†ç»„
    risk_score, risk_cuts, risk_labels = risk_minimization_grouping(male_fetus_data, k)
    algorithms_results.append((k, 'RiskMin', risk_score, risk_cuts, risk_labels))

# é€‰æ‹©æœ€ä½³ç®—æ³•
best_algorithm = max(algorithms_results, key=lambda x: x[2])
_, best_method, best_score, best_cuts, best_labels = best_algorithm

male_fetus_data['BMIèšç±»'] = best_labels

print(f"\n=== æœ€ä¼˜æ–¹æ¡ˆé€‰æ‹© ===")
print(f"æœ€ä½³ç®—æ³•: {best_method}")
print(f"æœ€ä½³åˆ†ç»„æ•°: {len(np.unique(best_labels))}")
print(f"æœ€ä½³è¯„åˆ†: {best_score:.4f}")

# === 7. åˆ†ç»„ç»“æœåˆ†æå’ŒNIPTæ—¶ç‚¹ä¼˜åŒ– ===

def optimize_nipt_timing(group_data):
    """
    ä¸ºæ¯ä¸ªç»„ä¼˜åŒ–NIPTæ—¶ç‚¹ï¼Œæœ€å°åŒ–é£é™©
    """
    if len(group_data) == 0:
        return np.nan, 0.0
    
    # å°è¯•ä¸åŒçš„æ£€æµ‹æ—¶ç‚¹ï¼ˆ10-20å‘¨ï¼‰
    test_weeks = np.arange(10, 21, 0.5)
    best_week = 13.0
    min_avg_risk = np.inf
    
    for week in test_weeks:
        # è®¡ç®—åœ¨æ­¤æ—¶ç‚¹æ£€æµ‹çš„å¹³å‡é£é™©
        simulated_risks = []
        for _, row in group_data.iterrows():
            risk = calculate_risk_score(
                row['å­•å¦‡BMIæŒ‡æ ‡'], 
                week,  # ä½¿ç”¨æµ‹è¯•æ—¶ç‚¹
                row['YæŸ“è‰²ä½“æµ“åº¦']
            )
            simulated_risks.append(risk)
        
        avg_risk = np.mean(simulated_risks)
        if avg_risk < min_avg_risk:
            min_avg_risk = avg_risk
            best_week = week
    
    # è®¡ç®—è¾¾æ ‡ç‡ï¼ˆYæŸ“è‰²ä½“æµ“åº¦>é˜ˆå€¼çš„æ¯”ä¾‹ï¼‰
    success_rate = np.mean(group_data['YæŸ“è‰²ä½“æµ“åº¦'] >= concentration_threshold)
    
    return best_week, success_rate * 100

# åˆ†æå„ç»„
summary_output = []
summary_output.append("=== T2 v2.0ï¼šåŸºäºç”Ÿå­˜åˆ†æçš„BMIåˆ†ç»„ç»“æœ ===\n")

# BMIç»Ÿè®¡
summary_output.append("--- 1. ç”·èƒå­•å¦‡BMIåˆ†å¸ƒåˆ†æ ---")
bmi_stats = male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].describe()
summary_output.append(f"BMIç»Ÿè®¡æè¿°:\n{bmi_stats}\n")

# åˆ†ç»„ç»“æœ
summary_output.append(f"--- 2. æœ€ä¼˜åˆ†ç»„æ–¹æ¡ˆ ---")
summary_output.append(f"ç®—æ³•: {best_method}")
summary_output.append(f"åˆ†ç»„æ•°: {len(np.unique(best_labels))}")
summary_output.append(f"è¯„åˆ†: {best_score:.4f}\n")

# å„ç»„åˆ†æ
summary_output.append("--- 3. å„BMIç»„è¯¦ç»†åˆ†æ ---")
group_results = []

for group_id in sorted(np.unique(best_labels)):
    group_data = male_fetus_data[male_fetus_data['BMIèšç±»'] == group_id]
    
    if len(group_data) > 0:
        bmi_min = group_data['å­•å¦‡BMIæŒ‡æ ‡'].min()
        bmi_max = group_data['å­•å¦‡BMIæŒ‡æ ‡'].max()
        sample_count = len(group_data)
        avg_risk = group_data['é£é™©è¯„åˆ†'].mean()
        
        # ä¼˜åŒ–NIPTæ—¶ç‚¹
        optimal_week, success_rate = optimize_nipt_timing(group_data)
        
        summary_output.append(f"èšç±» {group_id}: BMIèŒƒå›´ [{bmi_min:.2f}, {bmi_max:.2f}], "
                            f"æ ·æœ¬æ•°: {sample_count}, å¹³å‡é£é™©: {avg_risk:.3f}")
        summary_output.append(f"  æœ€ä¼˜NIPTæ—¶ç‚¹: {optimal_week:.2f}å‘¨, é¢„æœŸæˆåŠŸç‡: {success_rate:.1f}%")
        
        group_results.append({
            'BMIèšç±»': group_id,
            'BMIèŒƒå›´ä¸‹é™': bmi_min,
            'BMIèŒƒå›´ä¸Šé™': bmi_max,
            'æ ·æœ¬æ•°': sample_count,
            'å¹³å‡é£é™©è¯„åˆ†': avg_risk,
            'æœ€ä¼˜NIPTæ—¶ç‚¹ (å‘¨)': optimal_week,
            'é¢„æœŸæˆåŠŸç‡ (%)': success_rate,
            'é£é™©ç­‰çº§': 'ä½' if avg_risk < 1.0 else 'ä¸­' if avg_risk < 2.0 else 'é«˜'
        })

summary_output.append(f"\n--- 4. é£é™©åˆ†ææ€»ç»“ ---")
total_avg_risk = male_fetus_data['é£é™©è¯„åˆ†'].mean()
summary_output.append(f"æ•´ä½“å¹³å‡é£é™©: {total_avg_risk:.3f}")

# ä¿å­˜ç»“æœ
summary_text = '\n'.join(summary_output)
with open(os.path.join(results_dir, 'T2_survival_analysis_summary.txt'), 'w', encoding='utf-8') as f:
    f.write(summary_text)

# ä¿å­˜Excelç»“æœ
results_df = pd.DataFrame(group_results)
excel_path = os.path.join(results_dir, 'T2_survival_grouping_results.xlsx')
results_df.to_excel(excel_path, index=False)

print(summary_text)

# === 8. å¯è§†åŒ–ç»“æœ ===

# 1. ç”Ÿå­˜åˆ†ææ›²çº¿
plt.figure(figsize=(15, 12))

# å­å›¾1ï¼šå„ç»„çš„Kaplan-Meierç”Ÿå­˜æ›²çº¿
plt.subplot(2, 3, 1)
kmf = KaplanMeierFitter()

for group_id in sorted(np.unique(best_labels)):
    group_data = male_fetus_data[male_fetus_data['BMIèšç±»'] == group_id]
    if len(group_data) >= 5:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
        kmf.fit(group_data['æ—¶é—´_å­•å‘¨'], group_data['äº‹ä»¶_è¾¾æ ‡'], 
                label=f'BMIç»„ {group_id} (n={len(group_data)})')
        kmf.plot(ax=plt.gca())

plt.title('å„BMIç»„YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡çš„ç”Ÿå­˜æ›²çº¿')
plt.xlabel('å­•å‘¨')
plt.ylabel('è¾¾æ ‡æ¦‚ç‡')
plt.legend()
plt.grid(alpha=0.3)

# å­å›¾2ï¼šé£é™©è¯„åˆ†åˆ†å¸ƒ
plt.subplot(2, 3, 2)
for group_id in sorted(np.unique(best_labels)):
    group_data = male_fetus_data[male_fetus_data['BMIèšç±»'] == group_id]
    plt.hist(group_data['é£é™©è¯„åˆ†'], alpha=0.6, label=f'BMIç»„ {group_id}', bins=20)

plt.title('å„ç»„é£é™©è¯„åˆ†åˆ†å¸ƒ')
plt.xlabel('é£é™©è¯„åˆ†')
plt.ylabel('é¢‘æ•°')
plt.legend()
plt.grid(alpha=0.3)

# å­å›¾3ï¼šBMIåˆ†å¸ƒä¸åˆ†ç»„
plt.subplot(2, 3, 3)
bmi_range = np.linspace(male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].min(), 
                       male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].max(), 100)
plt.hist(male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'], bins=30, alpha=0.7, color='skyblue', label='BMIåˆ†å¸ƒ')

# æ ‡è®°åˆ†ç»„è¾¹ç•Œ
if len(best_cuts) > 0:
    for cut in best_cuts:
        plt.axvline(cut, color='red', linestyle='--', alpha=0.8)

plt.title('BMIåˆ†å¸ƒä¸åˆ†ç»„è¾¹ç•Œ')
plt.xlabel('BMI (kg/mÂ²)')
plt.ylabel('é¢‘æ•°')
plt.legend()
plt.grid(alpha=0.3)

# å­å›¾4ï¼šå„ç»„NIPTæ—¶ç‚¹æ¨è
plt.subplot(2, 3, 4)
group_ids = []
nipt_times = []
success_rates = []

for result in group_results:
    group_ids.append(f"ç»„{result['BMIèšç±»']}")
    nipt_times.append(result['æœ€ä¼˜NIPTæ—¶ç‚¹ (å‘¨)'])
    success_rates.append(result['é¢„æœŸæˆåŠŸç‡ (%)'])

bars = plt.bar(group_ids, nipt_times, alpha=0.8, color='lightcoral')
plt.title('å„ç»„æœ€ä¼˜NIPTæ—¶ç‚¹')
plt.xlabel('BMIç»„')
plt.ylabel('æ¨èå­•å‘¨')

# æ·»åŠ æˆåŠŸç‡æ ‡ç­¾
for bar, rate in zip(bars, success_rates):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
            f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')

plt.grid(alpha=0.3)

# å­å›¾5ï¼šç®—æ³•æ€§èƒ½å¯¹æ¯”
plt.subplot(2, 3, 5)
method_names = []
method_scores = []

for result in algorithms_results:
    k, method, score, _, _ = result
    method_names.append(f'{method}_K{k}')
    method_scores.append(score)

# æ·»åŠ ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
method_names.append('KMeans_best')
method_scores.append(max(silhouette_scores_kmeans, key=lambda x: x[1])[1])

plt.bar(range(len(method_names)), method_scores, alpha=0.8)
plt.title('ç®—æ³•æ€§èƒ½å¯¹æ¯”')
plt.xlabel('ç®—æ³•')
plt.ylabel('è¯„åˆ†')
plt.xticks(range(len(method_names)), method_names, rotation=45)
plt.grid(alpha=0.3)

# å­å›¾6ï¼šé£é™© vs BMIæ•£ç‚¹å›¾
plt.subplot(2, 3, 6)
colors = plt.cm.Set1(np.linspace(0, 1, len(np.unique(best_labels))))
for i, group_id in enumerate(sorted(np.unique(best_labels))):
    group_data = male_fetus_data[male_fetus_data['BMIèšç±»'] == group_id]
    plt.scatter(group_data['å­•å¦‡BMIæŒ‡æ ‡'], group_data['é£é™©è¯„åˆ†'], 
               c=[colors[i]], label=f'BMIç»„ {group_id}', alpha=0.6)

plt.title('BMI vs é£é™©è¯„åˆ†')
plt.xlabel('BMI (kg/mÂ²)')
plt.ylabel('é£é™©è¯„åˆ†')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(results_dir, 'T2_survival_analysis_comprehensive.png'), 
            dpi=500, bbox_inches='tight')
plt.close()

print(f"\nâœ… T2 v2.0 ç”Ÿå­˜åˆ†æå®Œæˆï¼")
print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {results_dir}")
print(f"ğŸ“ˆ æœ€ä½³ç®—æ³•: {best_method}")
print(f"ğŸ¯ åˆ†ç»„è¯„åˆ†: {best_score:.4f}")
print(f"âš¡ æ•´ä½“é£é™©ä¼˜åŒ–: {((2.0 - total_avg_risk) / 2.0 * 100):.1f}%")
