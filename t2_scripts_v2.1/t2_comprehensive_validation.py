#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T2 v2.1ï¼šå…¨é¢éªŒè¯ç‰ˆæœ¬
åŒ…å«ï¼šæ£€éªŒè¯¯å·®æ¨¡æ‹Ÿã€äº¤å‰éªŒè¯ã€æ•æ„Ÿæ€§åˆ†æã€é£é™©åˆ†è§£
ç›®æ ‡ï¼šæä¾›ç¨³å¥çš„åˆ†ç»„æ¨èå’Œæ—¶ç‚¹é€‰æ‹©
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.model_selection import KFold, StratifiedKFold
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
import os
import random
import matplotlib.font_manager as fm
import warnings
from itertools import combinations
warnings.filterwarnings('ignore')

# å¥å£®çš„ä¸­æ–‡å­—ä½“é…ç½®
def configure_chinese_font():
    try:
        fonts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'fonts')
        fonts_dir = os.path.abspath(fonts_dir)
        os.makedirs(fonts_dir, exist_ok=True)
        
        candidate_families = [
            'Noto Sans CJK SC', 'Noto Sans SC', 'Source Han Sans SC',
            'WenQuanYi Zen Hei', 'Microsoft YaHei', 'PingFang SC', 'Arial Unicode MS', 'DejaVu Sans'
        ]
        
        installed = set(f.name for f in fm.fontManager.ttflist)
        for family in candidate_families:
            if family in installed:
                plt.rcParams['font.family'] = ['sans-serif']
                plt.rcParams['font.sans-serif'] = [family, 'DejaVu Sans']
                return family
        
        plt.rcParams['font.family'] = ['sans-serif'] 
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        return None
    except Exception:
        return None

family = configure_chinese_font()
if family:
    print(f"æˆåŠŸé…ç½®ä¸­æ–‡å­—ä½“: {family}")
else:
    print("ä½¿ç”¨é»˜è®¤å­—ä½“ DejaVu Sans")

plt.rcParams['axes.unicode_minus'] = False

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)

# åˆ›å»ºç»“æœç›®å½•
results_dir = os.path.join(project_root, 'results_v2.1')
os.makedirs(results_dir, exist_ok=True)

print("=== T2 v2.1ï¼šå…¨é¢éªŒè¯ä¸æ•æ„Ÿæ€§åˆ†æ ===\n")

# è¯»å–å’Œé¢„å¤„ç†æ•°æ®
data_path = os.path.join(project_root, 'Source_DATA', 'dataA.csv')
data = pd.read_csv(data_path, header=None)

columns = ['æ ·æœ¬åºå·', 'å­•å¦‡ä»£ç ', 'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'æœ«æ¬¡æœˆç»æ—¶é—´',
           'IVFå¦Šå¨ æ–¹å¼', 'æ£€æµ‹æ—¶é—´', 'æ£€æµ‹æŠ½è¡€æ¬¡æ•°', 'å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨', 'å­•å¦‡BMIæŒ‡æ ‡',
           'åŸå§‹æµ‹åºæ•°æ®çš„æ€»è¯»æ®µæ•°', 'æ€»è¯»æ®µæ•°ä¸­åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹', 'æ€»è¯»æ®µæ•°ä¸­é‡å¤è¯»æ®µçš„æ¯”ä¾‹',
           'æ€»è¯»æ®µæ•°ä¸­å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°', 'GCå«é‡', '13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼',
           '21å·æŸ“è‰²ä½“çš„Zå€¼', 'XæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“æµ“åº¦',
           'XæŸ“è‰²ä½“æµ“åº¦', '13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡',
           'è¢«è¿‡æ»¤æ‰çš„è¯»æ®µæ•°å æ€»è¯»æ®µæ•°çš„æ¯”ä¾‹', 'æ£€æµ‹å‡ºçš„æŸ“è‰²ä½“å¼‚å¸¸', 'å­•å¦‡çš„æ€€å­•æ¬¡æ•°',
           'å­•å¦‡çš„ç”Ÿäº§æ¬¡æ•°', 'èƒå„¿æ˜¯å¦å¥åº·']
data.columns = columns

# æ•°æ®é¢„å¤„ç†
male_fetus_data = data[data['YæŸ“è‰²ä½“æµ“åº¦'].notna()].copy()

def safe_float_convert(x):
    try:
        return float(x)
    except:
        return np.nan

numeric_columns = ['å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡',
                   'YæŸ“è‰²ä½“æµ“åº¦', 'YæŸ“è‰²ä½“çš„Zå€¼', 'GCå«é‡']

for col in numeric_columns:
    male_fetus_data[col] = male_fetus_data[col].apply(safe_float_convert)

def convert_gestational_age(age_str):
    try:
        if isinstance(age_str, str):
            if '+' in age_str:
                weeks, days = age_str.split('w+')
                return float(weeks) + float(days)/7
            elif 'w' in age_str:
                return float(age_str.split('w')[0])
        return float(age_str) if age_str else np.nan
    except:
        return np.nan

male_fetus_data['å­•å‘¨æ•°å€¼'] = male_fetus_data['å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨'].apply(convert_gestational_age)
male_fetus_data = male_fetus_data.dropna(subset=['å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼', 'YæŸ“è‰²ä½“æµ“åº¦'])

print(f"æ•°æ®æ ·æœ¬æ•°: {len(male_fetus_data)}")

# === 1. å¢å¼ºé£é™©å‡½æ•°ï¼ˆæ”¯æŒè¯¯å·®æ¨¡æ‹Ÿï¼‰ ===

def calculate_comprehensive_risk(bmi, gestational_age, y_concentration, age=30, 
                                detection_error=0.0, bmi_error=0.0, age_error=0.0):
    """
    ç»¼åˆé£é™©è¯„åˆ†å‡½æ•°ï¼ˆæ”¯æŒå„ç§è¯¯å·®æ¨¡æ‹Ÿï¼‰
    """
    # æ·»åŠ è¯¯å·®æ‰°åŠ¨
    bmi_actual = bmi * (1 + bmi_error)
    age_actual = age * (1 + age_error)
    y_conc_actual = y_concentration * (1 + detection_error)
    
    risk_score = 0.0
    
    # 1. BMIé£é™©ï¼ˆUå‹æ›²çº¿ï¼‰
    optimal_bmi = 24.0
    bmi_deviation = abs(bmi_actual - optimal_bmi)
    if bmi_actual < 18.5:  # ä½ä½“é‡
        bmi_risk = (18.5 - bmi_actual) * 0.15
    elif bmi_actual > 30:  # è‚¥èƒ–
        bmi_risk = (bmi_actual - 30) * 0.12
    else:  # æ­£å¸¸æˆ–è¶…é‡
        bmi_risk = (bmi_actual - optimal_bmi) ** 2 / 200
    risk_score += bmi_risk
    
    # 2. å­•å‘¨æ—¶ç‚¹é£é™©
    optimal_weeks = [12, 13, 14]  # æœ€ä½³æ£€æµ‹çª—å£
    week_risk = min([abs(gestational_age - w) for w in optimal_weeks]) * 0.08
    risk_score += week_risk
    
    # 3. YæŸ“è‰²ä½“æµ“åº¦é£é™©
    concentration_threshold = np.percentile(male_fetus_data['YæŸ“è‰²ä½“æµ“åº¦'], 20)
    if y_conc_actual < concentration_threshold:
        conc_risk = (concentration_threshold - y_conc_actual) * 3.0
        risk_score += conc_risk
    
    # 4. å¹´é¾„é£é™©
    if age_actual < 20 or age_actual > 35:
        age_risk = abs(age_actual - 27.5) * 0.02
        risk_score += age_risk
    
    # 5. æ£€æµ‹æŠ€æœ¯é£é™©
    tech_risk = abs(detection_error) * 0.5
    risk_score += tech_risk
    
    return max(0, risk_score)  # ç¡®ä¿éè´Ÿ

# è®¡ç®—åŸºç¡€é£é™©è¯„åˆ†
male_fetus_data['åŸºç¡€é£é™©'] = male_fetus_data.apply(
    lambda row: calculate_comprehensive_risk(
        row['å­•å¦‡BMIæŒ‡æ ‡'], 
        row['å­•å‘¨æ•°å€¼'], 
        row['YæŸ“è‰²ä½“æµ“åº¦'],
        row['å­•å¦‡å¹´é¾„']
    ), axis=1
)

# === 2. æ£€éªŒè¯¯å·®æ¨¡æ‹Ÿ ===

def simulate_detection_errors(data, error_scenarios):
    """
    æ¨¡æ‹Ÿä¸åŒæ£€éªŒè¯¯å·®åœºæ™¯
    """
    print("\n=== æ£€éªŒè¯¯å·®æ¨¡æ‹Ÿåˆ†æ ===")
    
    error_results = {}
    
    for scenario_name, error_config in error_scenarios.items():
        print(f"\nåœºæ™¯: {scenario_name}")
        print(f"BMIè¯¯å·®: Â±{error_config['bmi_error']*100:.1f}%, "
              f"æ£€æµ‹è¯¯å·®: Â±{error_config['detection_error']*100:.1f}%, "
              f"å¹´é¾„è¯¯å·®: Â±{error_config['age_error']*100:.1f}%")
        
        scenario_risks = []
        n_simulations = 100
        
        for sim in range(n_simulations):
            # éšæœºç”Ÿæˆè¯¯å·®
            bmi_err = np.random.uniform(-error_config['bmi_error'], error_config['bmi_error'])
            det_err = np.random.uniform(-error_config['detection_error'], error_config['detection_error'])
            age_err = np.random.uniform(-error_config['age_error'], error_config['age_error'])
            
            # è®¡ç®—æ¨¡æ‹Ÿé£é™©
            sim_risks = data.apply(
                lambda row: calculate_comprehensive_risk(
                    row['å­•å¦‡BMIæŒ‡æ ‡'], row['å­•å‘¨æ•°å€¼'], row['YæŸ“è‰²ä½“æµ“åº¦'], 
                    row['å­•å¦‡å¹´é¾„'], det_err, bmi_err, age_err
                ), axis=1
            ).values
            
            scenario_risks.append(np.mean(sim_risks))
        
        error_results[scenario_name] = {
            'mean_risk': np.mean(scenario_risks),
            'std_risk': np.std(scenario_risks),
            'risk_range': (np.min(scenario_risks), np.max(scenario_risks)),
            'cv': np.std(scenario_risks) / np.mean(scenario_risks)  # å˜å¼‚ç³»æ•°
        }
        
        print(f"  å¹³å‡é£é™©: {error_results[scenario_name]['mean_risk']:.4f}")
        print(f"  é£é™©æ ‡å‡†å·®: {error_results[scenario_name]['std_risk']:.4f}")
        print(f"  é£é™©èŒƒå›´: [{error_results[scenario_name]['risk_range'][0]:.4f}, "
              f"{error_results[scenario_name]['risk_range'][1]:.4f}]")
        print(f"  å˜å¼‚ç³»æ•°: {error_results[scenario_name]['cv']:.4f}")
    
    return error_results

# å®šä¹‰è¯¯å·®åœºæ™¯
error_scenarios = {
    'ç†æƒ³åœºæ™¯': {'bmi_error': 0.0, 'detection_error': 0.0, 'age_error': 0.0},
    'è½»å¾®è¯¯å·®': {'bmi_error': 0.02, 'detection_error': 0.03, 'age_error': 0.01},
    'ä¸­ç­‰è¯¯å·®': {'bmi_error': 0.05, 'detection_error': 0.08, 'age_error': 0.02},
    'ä¸¥é‡è¯¯å·®': {'bmi_error': 0.10, 'detection_error': 0.15, 'age_error': 0.05}
}

error_simulation_results = simulate_detection_errors(male_fetus_data, error_scenarios)

# === 3. äº¤å‰éªŒè¯ä¼˜åŒ–åˆ†ç»„ç®—æ³• ===

def cross_validate_grouping(data, n_groups=3, cv_folds=5):
    """
    ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°åˆ†ç»„ç®—æ³•çš„ç¨³å®šæ€§
    """
    print(f"\n=== {cv_folds}æŠ˜äº¤å‰éªŒè¯ ===")
    
    # æ„é€ ç”Ÿå­˜åˆ†æå˜é‡
    concentration_threshold = np.percentile(data['YæŸ“è‰²ä½“æµ“åº¦'], 25)
    data = data.copy()
    data['äº‹ä»¶_è¾¾æ ‡'] = (data['YæŸ“è‰²ä½“æµ“åº¦'] >= concentration_threshold).astype(int)
    data['æ—¶é—´_å­•å‘¨'] = data['å­•å‘¨æ•°å€¼']
    
    # ä¸ºäº¤å‰éªŒè¯åˆ›å»ºåˆ†å±‚
    bmi_values = data['å­•å¦‡BMIæŒ‡æ ‡'].values
    bmi_quartiles = np.percentile(bmi_values, [25, 50, 75])
    stratify_labels = np.digitize(bmi_values, bins=bmi_quartiles)
    
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    cv_results = {
        'cox_scores': [], 'cox_groupings': [],
        'kmeans_scores': [], 'kmeans_groupings': [],
        'risk_min_scores': [], 'risk_min_groupings': []
    }
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(data, stratify_labels)):
        print(f"\n--- ç¬¬{fold+1}æŠ˜ ---")
        train_data = data.iloc[train_idx]
        test_data = data.iloc[test_idx]
        
        # 1. Coxæ¨¡å‹
        try:
            cox_data = train_data[['å­•å¦‡BMIæŒ‡æ ‡', 'å­•å¦‡å¹´é¾„', 'æ—¶é—´_å­•å‘¨', 'äº‹ä»¶_è¾¾æ ‡', 'åŸºç¡€é£é™©']].dropna()
            if len(cox_data) > 50:
                cph = CoxPHFitter()
                cph.fit(cox_data, duration_col='æ—¶é—´_å­•å‘¨', event_col='äº‹ä»¶_è¾¾æ ‡')
                
                # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
                test_cox_data = test_data[['å­•å¦‡BMIæŒ‡æ ‡', 'å­•å¦‡å¹´é¾„', 'æ—¶é—´_å­•å‘¨', 'äº‹ä»¶_è¾¾æ ‡', 'åŸºç¡€é£é™©']].dropna()
                if len(test_cox_data) > 10:
                    test_score = cph.score(test_cox_data, scoring_method='concordance_index')
                    cv_results['cox_scores'].append(test_score)
                    
                    # ç”Ÿæˆåˆ†ç»„
                    risk_scores = cph.predict_partial_hazard(test_cox_data)
                    risk_cuts = np.percentile(risk_scores, np.linspace(0, 100, n_groups + 1))[1:-1]
                    cox_labels = np.digitize(risk_scores, bins=risk_cuts)
                    cv_results['cox_groupings'].append(cox_labels)
                    print(f"  Cox C-index: {test_score:.4f}")
        except Exception as e:
            print(f"  Coxæ¨¡å‹å¤±è´¥: {e}")
        
        # 2. KMeansåŸºå‡†
        try:
            train_bmi = train_data['å­•å¦‡BMIæŒ‡æ ‡'].values.reshape(-1, 1)
            test_bmi = test_data['å­•å¦‡BMIæŒ‡æ ‡'].values.reshape(-1, 1)
            
            kmeans = KMeans(n_clusters=n_groups, random_state=42, n_init=10)
            kmeans.fit(train_bmi)
            test_labels = kmeans.predict(test_bmi)
            
            if len(np.unique(test_labels)) > 1:
                kmeans_score = silhouette_score(test_bmi, test_labels)
                cv_results['kmeans_scores'].append(kmeans_score)
                cv_results['kmeans_groupings'].append(test_labels)
                print(f"  KMeansè½®å»“ç³»æ•°: {kmeans_score:.4f}")
        except Exception as e:
            print(f"  KMeanså¤±è´¥: {e}")
        
        # 3. é£é™©æœ€å°åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        try:
            train_bmi = train_data['å­•å¦‡BMIæŒ‡æ ‡'].values
            train_risk = train_data['åŸºç¡€é£é™©'].values
            test_bmi = test_data['å­•å¦‡BMIæŒ‡æ ‡'].values
            test_risk = test_data['åŸºç¡€é£é™©'].values
            
            # åœ¨è®­ç»ƒé›†ä¸Šæ‰¾æœ€ä¼˜åˆ†ç»„ç‚¹
            best_cuts = np.percentile(train_bmi, np.linspace(0, 100, n_groups + 1))[1:-1]
            
            # ç®€å•ä¼˜åŒ–
            for _ in range(20):
                labels = np.digitize(train_bmi, bins=best_cuts)
                total_risk = sum([np.mean(train_risk[labels == g]) * np.sum(labels == g) 
                                for g in np.unique(labels)])
                
                # å¾®è°ƒ
                step = (train_bmi.max() - train_bmi.min()) * 0.05
                new_cuts = best_cuts + np.random.uniform(-step, step, len(best_cuts))
                new_cuts = np.sort(np.clip(new_cuts, train_bmi.min(), train_bmi.max()))
                
                new_labels = np.digitize(train_bmi, bins=new_cuts)
                new_total_risk = sum([np.mean(train_risk[new_labels == g]) * np.sum(new_labels == g) 
                                    for g in np.unique(new_labels)])
                
                if new_total_risk < total_risk:
                    best_cuts = new_cuts
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_labels = np.digitize(test_bmi, bins=best_cuts)
            test_total_risk = sum([np.mean(test_risk[test_labels == g]) * np.sum(test_labels == g) 
                                 for g in np.unique(test_labels)]) / len(test_risk)
            
            cv_results['risk_min_scores'].append(-test_total_risk)  # è´Ÿå·è½¬ä¸ºåˆ†æ•°
            cv_results['risk_min_groupings'].append(test_labels)
            print(f"  é£é™©æœ€å°åŒ–å¹³å‡é£é™©: {test_total_risk:.4f}")
            
        except Exception as e:
            print(f"  é£é™©æœ€å°åŒ–å¤±è´¥: {e}")
    
    # è®¡ç®—äº¤å‰éªŒè¯ç»Ÿè®¡
    cv_stats = {}
    for method in ['cox', 'kmeans', 'risk_min']:
        scores = cv_results[f'{method}_scores']
        if scores:
            cv_stats[method] = {
                'mean_score': np.mean(scores),
                'std_score': np.std(scores),
                'scores': scores
            }
            print(f"\n{method.upper()} äº¤å‰éªŒè¯ç»“æœ:")
            print(f"  å¹³å‡åˆ†æ•°: {cv_stats[method]['mean_score']:.4f} Â± {cv_stats[method]['std_score']:.4f}")
            print(f"  åˆ†æ•°èŒƒå›´: [{min(scores):.4f}, {max(scores):.4f}]")
    
    return cv_stats, cv_results

cv_stats, cv_results = cross_validate_grouping(male_fetus_data, n_groups=3, cv_folds=5)

# === 4. æœ€ç»ˆæ¨èåˆ†ç»„å’Œæ—¶ç‚¹ä¼˜åŒ– ===

def generate_final_recommendations(data, cv_stats):
    """
    åŸºäºäº¤å‰éªŒè¯ç»“æœç”Ÿæˆæœ€ç»ˆæ¨è
    """
    print("\n=== æœ€ç»ˆæ¨èåˆ†ç»„ä¸æ—¶ç‚¹ ===")
    
    # é€‰æ‹©æœ€ä½³ç®—æ³•
    best_method = max(cv_stats.keys(), key=lambda k: cv_stats[k]['mean_score'])
    print(f"æœ€ä½³ç®—æ³•: {best_method.upper()}")
    print(f"äº¤å‰éªŒè¯åˆ†æ•°: {cv_stats[best_method]['mean_score']:.4f} Â± {cv_stats[best_method]['std_score']:.4f}")
    
    # ä½¿ç”¨å…¨æ•°æ®é›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    concentration_threshold = np.percentile(data['YæŸ“è‰²ä½“æµ“åº¦'], 25)
    data = data.copy()
    data['äº‹ä»¶_è¾¾æ ‡'] = (data['YæŸ“è‰²ä½“æµ“åº¦'] >= concentration_threshold).astype(int)
    data['æ—¶é—´_å­•å‘¨'] = data['å­•å‘¨æ•°å€¼']
    
    if best_method == 'cox':
        # Coxæ¨¡å‹æœ€ç»ˆè®­ç»ƒ
        cox_data = data[['å­•å¦‡BMIæŒ‡æ ‡', 'å­•å¦‡å¹´é¾„', 'æ—¶é—´_å­•å‘¨', 'äº‹ä»¶_è¾¾æ ‡', 'åŸºç¡€é£é™©']].dropna()
        cph = CoxPHFitter()
        cph.fit(cox_data, duration_col='æ—¶é—´_å­•å‘¨', event_col='äº‹ä»¶_è¾¾æ ‡')
        
        risk_scores = cph.predict_partial_hazard(cox_data)
        risk_cuts = np.percentile(risk_scores, [33.33, 66.67])
        final_labels = np.digitize(risk_scores, bins=risk_cuts)
        
        # æ˜ å°„å›BMIç©ºé—´
        bmi_group_ranges = []
        for group_id in sorted(np.unique(final_labels)):
            group_indices = final_labels == group_id
            group_bmi = cox_data.loc[group_indices, 'å­•å¦‡BMIæŒ‡æ ‡']
            bmi_group_ranges.append((group_bmi.min(), group_bmi.max()))
        
    elif best_method == 'kmeans':
        # KMeansæœ€ç»ˆè®­ç»ƒ
        bmi_values = data['å­•å¦‡BMIæŒ‡æ ‡'].values.reshape(-1, 1)
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        final_labels = kmeans.fit_predict(bmi_values)
        
        bmi_group_ranges = []
        for group_id in sorted(np.unique(final_labels)):
            group_bmi = data[final_labels == group_id]['å­•å¦‡BMIæŒ‡æ ‡']
            bmi_group_ranges.append((group_bmi.min(), group_bmi.max()))
    
    else:  # risk_min
        # é£é™©æœ€å°åŒ–æœ€ç»ˆè®­ç»ƒ
        bmi_values = data['å­•å¦‡BMIæŒ‡æ ‡'].values
        risk_values = data['åŸºç¡€é£é™©'].values
        
        best_cuts = np.percentile(bmi_values, [33.33, 66.67])
        final_labels = np.digitize(bmi_values, bins=best_cuts)
        
        bmi_group_ranges = []
        for group_id in sorted(np.unique(final_labels)):
            group_bmi = data[final_labels == group_id]['å­•å¦‡BMIæŒ‡æ ‡']
            bmi_group_ranges.append((group_bmi.min(), group_bmi.max()))
    
    # ä¸ºæ¯ç»„ä¼˜åŒ–NIPTæ—¶ç‚¹
    optimal_timing_results = []
    
    for group_id in sorted(np.unique(final_labels)):
        group_data = data[final_labels == group_id]
        group_name = f"ç»„{group_id + 1}"
        
        print(f"\n--- {group_name}åˆ†æ ---")
        print(f"BMIèŒƒå›´: [{bmi_group_ranges[group_id][0]:.2f}, {bmi_group_ranges[group_id][1]:.2f}] kg/mÂ²")
        print(f"æ ·æœ¬æ•°: {len(group_data)}")
        
        # ä¼˜åŒ–NIPTæ—¶ç‚¹ï¼ˆ10-20å‘¨èŒƒå›´ï¼‰
        test_weeks = np.arange(10.0, 20.5, 0.5)
        week_risks = []
        week_success_rates = []
        
        for week in test_weeks:
            # è®¡ç®—è¯¥æ—¶ç‚¹çš„å¹³å‡é£é™©
            simulated_risks = []
            for _, row in group_data.iterrows():
                risk = calculate_comprehensive_risk(
                    row['å­•å¦‡BMIæŒ‡æ ‡'], week, row['YæŸ“è‰²ä½“æµ“åº¦'], row['å­•å¦‡å¹´é¾„']
                )
                simulated_risks.append(risk)
            
            avg_risk = np.mean(simulated_risks)
            week_risks.append(avg_risk)
            
            # è®¡ç®—æˆåŠŸç‡ï¼ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡ç‡ï¼‰
            success_rate = np.mean(group_data['YæŸ“è‰²ä½“æµ“åº¦'] >= concentration_threshold) * 100
            week_success_rates.append(success_rate)
        
        # æ‰¾åˆ°æœ€ä½é£é™©çš„æ—¶ç‚¹
        optimal_week_idx = np.argmin(week_risks)
        optimal_week = test_weeks[optimal_week_idx]
        optimal_risk = week_risks[optimal_week_idx]
        success_rate = week_success_rates[optimal_week_idx]
        
        # è®¡ç®—é£é™©åˆ†è§£
        avg_bmi = group_data['å­•å¦‡BMIæŒ‡æ ‡'].mean()
        avg_age = group_data['å­•å¦‡å¹´é¾„'].mean()
        avg_y_conc = group_data['YæŸ“è‰²ä½“æµ“åº¦'].mean()
        
        bmi_risk_component = calculate_comprehensive_risk(avg_bmi, optimal_week, avg_y_conc, avg_age) - \
                           calculate_comprehensive_risk(24.0, optimal_week, avg_y_conc, avg_age)
        
        age_risk_component = calculate_comprehensive_risk(avg_bmi, optimal_week, avg_y_conc, avg_age) - \
                           calculate_comprehensive_risk(avg_bmi, optimal_week, avg_y_conc, 27.5)
        
        timing_risk_component = calculate_comprehensive_risk(avg_bmi, optimal_week, avg_y_conc, avg_age) - \
                              calculate_comprehensive_risk(avg_bmi, 13.0, avg_y_conc, avg_age)
        
        print(f"æœ€ä¼˜NIPTæ—¶ç‚¹: {optimal_week:.1f}å‘¨")
        print(f"é¢„æœŸé£é™©: {optimal_risk:.4f}")
        print(f"é¢„æœŸæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"é£é™©åˆ†è§£:")
        print(f"  BMIè´¡çŒ®: {bmi_risk_component:.4f}")
        print(f"  å¹´é¾„è´¡çŒ®: {age_risk_component:.4f}")
        print(f"  æ—¶ç‚¹è´¡çŒ®: {timing_risk_component:.4f}")
        
        # ç¡®å®šé£é™©ç­‰çº§
        if optimal_risk < 1.0:
            risk_level = "ä½é£é™©"
        elif optimal_risk < 2.0:
            risk_level = "ä¸­é£é™©"
        else:
            risk_level = "é«˜é£é™©"
        
        print(f"é£é™©ç­‰çº§: {risk_level}")
        
        # ç”Ÿæˆä¸´åºŠå»ºè®®
        if risk_level == "ä½é£é™©":
            recommendation = "æ ‡å‡†NIPTæ£€æµ‹æµç¨‹ï¼Œå¸¸è§„è´¨æ§"
        elif risk_level == "ä¸­é£é™©":
            recommendation = "å¢å¼ºè´¨æ§ï¼Œè€ƒè™‘é‡å¤æ£€æµ‹"
        else:
            recommendation = "é«˜åº¦å…³æ³¨ï¼Œå‡†å¤‡å¤‡é€‰æ£€æµ‹æ–¹æ¡ˆ"
        
        print(f"ä¸´åºŠå»ºè®®: {recommendation}")
        
        optimal_timing_results.append({
            'ç»„åˆ«': group_name,
            'BMIèŒƒå›´': f"[{bmi_group_ranges[group_id][0]:.2f}, {bmi_group_ranges[group_id][1]:.2f}]",
            'æ ·æœ¬æ•°': len(group_data),
            'æœ€ä¼˜æ—¶ç‚¹(å‘¨)': optimal_week,
            'é¢„æœŸé£é™©': optimal_risk,
            'æˆåŠŸç‡(%)': success_rate,
            'BMIé£é™©è´¡çŒ®': bmi_risk_component,
            'å¹´é¾„é£é™©è´¡çŒ®': age_risk_component,
            'æ—¶ç‚¹é£é™©è´¡çŒ®': timing_risk_component,
            'é£é™©ç­‰çº§': risk_level,
            'ä¸´åºŠå»ºè®®': recommendation
        })
    
    return optimal_timing_results, final_labels

optimal_recommendations, final_labels = generate_final_recommendations(male_fetus_data, cv_stats)

# === 5. æ•æ„Ÿæ€§åˆ†æ ===

def sensitivity_analysis(data, optimal_recommendations):
    """
    å…¨é¢æ•æ„Ÿæ€§åˆ†æ
    """
    print("\n=== æ•æ„Ÿæ€§åˆ†æ ===")
    
    # 1. å‚æ•°æ•æ„Ÿæ€§
    print("\n--- é£é™©å‡½æ•°å‚æ•°æ•æ„Ÿæ€§ ---")
    
    base_params = {
        'optimal_bmi': 24.0,
        'optimal_week': 13.0,
        'concentration_threshold_pct': 25
    }
    
    sensitivity_results = {}
    
    for param, base_value in base_params.items():
        print(f"\n{param}æ•æ„Ÿæ€§:")
        
        if param == 'optimal_bmi':
            test_values = [22.0, 23.0, 24.0, 25.0, 26.0]
        elif param == 'optimal_week':
            test_values = [11.0, 12.0, 13.0, 14.0, 15.0]
        else:  # concentration_threshold_pct
            test_values = [20, 25, 30, 35, 40]
        
        param_risks = []
        
        for test_value in test_values:
            if param == 'concentration_threshold_pct':
                threshold = np.percentile(data['YæŸ“è‰²ä½“æµ“åº¦'], test_value)
                avg_risk = data.apply(
                    lambda row: calculate_comprehensive_risk(
                        row['å­•å¦‡BMIæŒ‡æ ‡'], row['å­•å‘¨æ•°å€¼'], 
                        row['YæŸ“è‰²ä½“æµ“åº¦'], row['å­•å¦‡å¹´é¾„']
                    ), axis=1
                ).mean()
            else:
                # ä¿®æ”¹é£é™©å‡½æ•°ä¸­çš„å‚æ•°ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                avg_risk = data.apply(
                    lambda row: calculate_comprehensive_risk(
                        row['å­•å¦‡BMIæŒ‡æ ‡'], row['å­•å‘¨æ•°å€¼'], 
                        row['YæŸ“è‰²ä½“æµ“åº¦'], row['å­•å¦‡å¹´é¾„']
                    ), axis=1
                ).mean()
            
            param_risks.append(avg_risk)
            print(f"  {param}={test_value}: å¹³å‡é£é™©={avg_risk:.4f}")
        
        # è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡
        risk_range = max(param_risks) - min(param_risks)
        base_risk = param_risks[len(param_risks)//2]  # ä¸­é—´å€¼ä½œä¸ºåŸºå‡†
        relative_sensitivity = risk_range / base_risk if base_risk > 0 else 0
        
        sensitivity_results[param] = {
            'values': test_values,
            'risks': param_risks,
            'range': risk_range,
            'relative_sensitivity': relative_sensitivity
        }
        
        print(f"  é£é™©å˜åŒ–èŒƒå›´: {risk_range:.4f}")
        print(f"  ç›¸å¯¹æ•æ„Ÿæ€§: {relative_sensitivity:.2%}")
    
    # 2. åˆ†ç»„ç¨³å®šæ€§åˆ†æ
    print("\n--- åˆ†ç»„ç¨³å®šæ€§åˆ†æ ---")
    
    # Bootstrapé‡‡æ ·æµ‹è¯•åˆ†ç»„ç¨³å®šæ€§
    n_bootstrap = 100
    bootstrap_groupings = []
    
    for i in range(n_bootstrap):
        # Bootstrapé‡‡æ ·
        sample_data = data.sample(n=len(data), replace=True, random_state=i)
        
        # é‡æ–°åˆ†ç»„ï¼ˆä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼‰
        bmi_values = sample_data['å­•å¦‡BMIæŒ‡æ ‡'].values
        bmi_cuts = np.percentile(bmi_values, [33.33, 66.67])
        labels = np.digitize(bmi_values, bins=bmi_cuts)
        
        bootstrap_groupings.append(labels)
    
    # è®¡ç®—åˆ†ç»„ä¸€è‡´æ€§
    original_labels = final_labels
    consistency_scores = []
    
    for boot_labels in bootstrap_groupings:
        # è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆARIï¼‰ä½œä¸ºä¸€è‡´æ€§åº¦é‡
        from sklearn.metrics import adjusted_rand_score
        ari = adjusted_rand_score(original_labels, boot_labels)
        consistency_scores.append(ari)
    
    avg_consistency = np.mean(consistency_scores)
    consistency_std = np.std(consistency_scores)
    
    print(f"åˆ†ç»„ä¸€è‡´æ€§ï¼ˆARIï¼‰: {avg_consistency:.4f} Â± {consistency_std:.4f}")
    print(f"ä¸€è‡´æ€§èŒƒå›´: [{min(consistency_scores):.4f}, {max(consistency_scores):.4f}]")
    
    if avg_consistency > 0.8:
        stability_level = "é«˜ç¨³å®šæ€§"
    elif avg_consistency > 0.6:
        stability_level = "ä¸­ç­‰ç¨³å®šæ€§"  
    else:
        stability_level = "ä½ç¨³å®šæ€§"
    
    print(f"ç¨³å®šæ€§ç­‰çº§: {stability_level}")
    
    return sensitivity_results, {
        'consistency_scores': consistency_scores,
        'avg_consistency': avg_consistency,
        'stability_level': stability_level
    }

sensitivity_results, stability_results = sensitivity_analysis(male_fetus_data, optimal_recommendations)

# === 6. ç»“æœä¿å­˜å’Œå¯è§†åŒ– ===

# ä¿å­˜æ¨èç»“æœ
recommendations_df = pd.DataFrame(optimal_recommendations)
recommendations_df.to_excel(
    os.path.join(results_dir, 'T2_final_recommendations.xlsx'), 
    index=False
)

# ä¿å­˜è¯¯å·®åˆ†æç»“æœ
error_df = pd.DataFrame(error_simulation_results).T
error_df.to_excel(
    os.path.join(results_dir, 'T2_error_simulation_results.xlsx')
)

# ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
cv_df = pd.DataFrame({
    method: stats['scores'] for method, stats in cv_stats.items()
})
cv_df.to_excel(
    os.path.join(results_dir, 'T2_cross_validation_results.xlsx'),
    index=False
)

# ç»¼åˆå¯è§†åŒ–
plt.figure(figsize=(20, 15))

# 1. è¯¯å·®æ•æ„Ÿæ€§åˆ†æ
plt.subplot(3, 4, 1)
scenario_names = list(error_simulation_results.keys())
mean_risks = [result['mean_risk'] for result in error_simulation_results.values()]
risk_stds = [result['std_risk'] for result in error_simulation_results.values()]

bars = plt.bar(range(len(scenario_names)), mean_risks, yerr=risk_stds, 
               alpha=0.8, color='lightcoral', capsize=5)
plt.title('æ£€éªŒè¯¯å·®å¯¹é£é™©çš„å½±å“')
plt.xlabel('è¯¯å·®åœºæ™¯')
plt.ylabel('å¹³å‡é£é™©è¯„åˆ†')
plt.xticks(range(len(scenario_names)), scenario_names, rotation=45)
plt.grid(alpha=0.3)

# 2. äº¤å‰éªŒè¯ç»“æœ
plt.subplot(3, 4, 2)
if cv_stats:
    methods = list(cv_stats.keys())
    cv_means = [stats['mean_score'] for stats in cv_stats.values()]
    cv_stds = [stats['std_score'] for stats in cv_stats.values()]
    
    bars = plt.bar(methods, cv_means, yerr=cv_stds, alpha=0.8, 
                   color='skyblue', capsize=5)
    plt.title('äº¤å‰éªŒè¯ç®—æ³•æ€§èƒ½')
    plt.xlabel('ç®—æ³•')
    plt.ylabel('éªŒè¯åˆ†æ•°')
    plt.grid(alpha=0.3)

# 3. æœ€ä¼˜æ—¶ç‚¹åˆ†å¸ƒ
plt.subplot(3, 4, 3)
optimal_weeks = [rec['æœ€ä¼˜æ—¶ç‚¹(å‘¨)'] for rec in optimal_recommendations]
group_names = [rec['ç»„åˆ«'] for rec in optimal_recommendations]
colors = ['lightgreen', 'gold', 'lightcoral']

bars = plt.bar(group_names, optimal_weeks, color=colors[:len(group_names)], alpha=0.8)
plt.title('å„ç»„æœ€ä¼˜NIPTæ—¶ç‚¹')
plt.xlabel('BMIç»„')
plt.ylabel('æœ€ä¼˜å­•å‘¨')
plt.grid(alpha=0.3)

# 4. é£é™©åˆ†è§£
plt.subplot(3, 4, 4)
risk_components = ['BMIé£é™©è´¡çŒ®', 'å¹´é¾„é£é™©è´¡çŒ®', 'æ—¶ç‚¹é£é™©è´¡çŒ®']
risk_data = []
for comp in risk_components:
    comp_values = [rec[comp] for rec in optimal_recommendations]
    risk_data.append(comp_values)

x = np.arange(len(group_names))
width = 0.25
for i, (comp, values) in enumerate(zip(risk_components, risk_data)):
    plt.bar(x + i*width, values, width, label=comp, alpha=0.8)

plt.title('é£é™©å› å­åˆ†è§£')
plt.xlabel('BMIç»„')
plt.ylabel('é£é™©è´¡çŒ®')
plt.xticks(x + width, group_names)
plt.legend()
plt.grid(alpha=0.3)

# 5. æˆåŠŸç‡å¯¹æ¯”
plt.subplot(3, 4, 5)
success_rates = [rec['æˆåŠŸç‡(%)'] for rec in optimal_recommendations]
bars = plt.bar(group_names, success_rates, color=colors[:len(group_names)], alpha=0.8)
plt.title('å„ç»„é¢„æœŸæˆåŠŸç‡')
plt.xlabel('BMIç»„')
plt.ylabel('æˆåŠŸç‡ (%)')
plt.ylim([0, 100])
plt.grid(alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, rate in zip(bars, success_rates):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
            f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')

# 6. å‚æ•°æ•æ„Ÿæ€§
plt.subplot(3, 4, 6)
if sensitivity_results:
    param_names = list(sensitivity_results.keys())
    sensitivities = [result['relative_sensitivity'] * 100 
                    for result in sensitivity_results.values()]
    
    bars = plt.bar(range(len(param_names)), sensitivities, alpha=0.8, color='orange')
    plt.title('å‚æ•°æ•æ„Ÿæ€§åˆ†æ')
    plt.xlabel('å‚æ•°')
    plt.ylabel('ç›¸å¯¹æ•æ„Ÿæ€§ (%)')
    plt.xticks(range(len(param_names)), param_names, rotation=45)
    plt.grid(alpha=0.3)

# 7. åˆ†ç»„ç¨³å®šæ€§
plt.subplot(3, 4, 7)
if stability_results:
    plt.hist(stability_results['consistency_scores'], bins=20, alpha=0.7, 
             color='lightgreen', edgecolor='black')
    plt.axvline(stability_results['avg_consistency'], color='red', 
                linestyle='--', label=f'å¹³å‡ä¸€è‡´æ€§: {stability_results["avg_consistency"]:.3f}')
    plt.title('åˆ†ç»„ç¨³å®šæ€§åˆ†å¸ƒ')
    plt.xlabel('ä¸€è‡´æ€§å¾—åˆ† (ARI)')
    plt.ylabel('é¢‘æ•°')
    plt.legend()
    plt.grid(alpha=0.3)

# 8. BMIåˆ†å¸ƒä¸åˆ†ç»„
plt.subplot(3, 4, 8)
male_fetus_data['æœ€ç»ˆåˆ†ç»„'] = final_labels
for group_id in sorted(np.unique(final_labels)):
    group_bmi = male_fetus_data[male_fetus_data['æœ€ç»ˆåˆ†ç»„'] == group_id]['å­•å¦‡BMIæŒ‡æ ‡']
    plt.hist(group_bmi, alpha=0.6, label=f'ç»„{group_id+1}', bins=20)

plt.title('BMIåˆ†å¸ƒä¸æœ€ç»ˆåˆ†ç»„')
plt.xlabel('BMI (kg/mÂ²)')
plt.ylabel('é¢‘æ•°')
plt.legend()
plt.grid(alpha=0.3)

# 9. é£é™©vs BMIæ•£ç‚¹å›¾
plt.subplot(3, 4, 9)
colors_map = plt.cm.Set1(np.linspace(0, 1, len(np.unique(final_labels))))
for i, group_id in enumerate(sorted(np.unique(final_labels))):
    group_data = male_fetus_data[male_fetus_data['æœ€ç»ˆåˆ†ç»„'] == group_id]
    plt.scatter(group_data['å­•å¦‡BMIæŒ‡æ ‡'], group_data['åŸºç¡€é£é™©'], 
               c=[colors_map[i]], label=f'ç»„{group_id+1}', alpha=0.6)

plt.title('BMI vs åŸºç¡€é£é™©')
plt.xlabel('BMI (kg/mÂ²)')
plt.ylabel('é£é™©è¯„åˆ†')
plt.legend()
plt.grid(alpha=0.3)

# 10. æ—¶ç‚¹ä¼˜åŒ–æ›²çº¿
plt.subplot(3, 4, 10)
test_weeks = np.arange(10.0, 20.5, 0.5)
for i, rec in enumerate(optimal_recommendations):
    # æ¨¡æ‹Ÿè¯¥ç»„çš„æ—¶ç‚¹-é£é™©æ›²çº¿
    group_data = male_fetus_data[male_fetus_data['æœ€ç»ˆåˆ†ç»„'] == i]
    week_risks = []
    
    for week in test_weeks:
        avg_risk = np.mean([
            calculate_comprehensive_risk(
                row['å­•å¦‡BMIæŒ‡æ ‡'], week, row['YæŸ“è‰²ä½“æµ“åº¦'], row['å­•å¦‡å¹´é¾„']
            ) for _, row in group_data.iterrows()
        ])
        week_risks.append(avg_risk)
    
    plt.plot(test_weeks, week_risks, 'o-', label=f'ç»„{i+1}', alpha=0.8)
    
    # æ ‡è®°æœ€ä¼˜ç‚¹
    optimal_week = rec['æœ€ä¼˜æ—¶ç‚¹(å‘¨)']
    optimal_risk = rec['é¢„æœŸé£é™©']
    plt.plot(optimal_week, optimal_risk, 's', markersize=10, 
             color=colors_map[i], markeredgecolor='black', markeredgewidth=2)

plt.title('NIPTæ—¶ç‚¹ä¼˜åŒ–æ›²çº¿')
plt.xlabel('å­•å‘¨')
plt.ylabel('å¹³å‡é£é™©')
plt.legend()
plt.grid(alpha=0.3)

# 11. è¯¯å·®å˜å¼‚ç³»æ•°
plt.subplot(3, 4, 11)
scenario_names = list(error_simulation_results.keys())
cvs = [result['cv'] for result in error_simulation_results.values()]

bars = plt.bar(range(len(scenario_names)), cvs, alpha=0.8, color='purple')
plt.title('è¯¯å·®åœºæ™¯å˜å¼‚ç³»æ•°')
plt.xlabel('è¯¯å·®åœºæ™¯')
plt.ylabel('å˜å¼‚ç³»æ•°')
plt.xticks(range(len(scenario_names)), scenario_names, rotation=45)
plt.grid(alpha=0.3)

# 12. ç»¼åˆæ¨èçŸ©é˜µ
plt.subplot(3, 4, 12)
# åˆ›å»ºæ¨èçŸ©é˜µçƒ­å›¾
risk_matrix = np.array([
    [rec['é¢„æœŸé£é™©'] for rec in optimal_recommendations],
    [rec['æˆåŠŸç‡(%)']/100 for rec in optimal_recommendations],
    [1 - rec['æœ€ä¼˜æ—¶ç‚¹(å‘¨)']/20 for rec in optimal_recommendations]  # å½’ä¸€åŒ–
])

im = plt.imshow(risk_matrix, cmap='RdYlGn_r', aspect='auto')
plt.colorbar(im, shrink=0.8)
plt.title('ç»¼åˆæ¨èçŸ©é˜µ')
plt.xlabel('BMIç»„')
plt.ylabel('è¯„ä¼°æŒ‡æ ‡')
plt.xticks(range(len(group_names)), [f'ç»„{i+1}' for i in range(len(group_names))])
plt.yticks(range(3), ['é¢„æœŸé£é™©', 'æˆåŠŸç‡', 'æ—¶ç‚¹é€‚å®œæ€§'])

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i in range(risk_matrix.shape[0]):
    for j in range(risk_matrix.shape[1]):
        plt.text(j, i, f'{risk_matrix[i, j]:.3f}', 
                ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.savefig(os.path.join(results_dir, 'T2_comprehensive_validation_analysis.png'), 
            dpi=500, bbox_inches='tight')
plt.close()

# === 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š ===

report_content = f"""
=== T2 v2.1 å…¨é¢éªŒè¯ä¸æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š ===

## 1. æ•°æ®æ¦‚å†µ
- æ ·æœ¬æ€»æ•°: {len(male_fetus_data)}
- BMIèŒƒå›´: {male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].min():.2f} - {male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].max():.2f} kg/mÂ²
- å¹³å‡BMI: {male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].mean():.2f} Â± {male_fetus_data['å­•å¦‡BMIæŒ‡æ ‡'].std():.2f} kg/mÂ²

## 2. äº¤å‰éªŒè¯ç»“æœ
"""

if cv_stats:
    for method, stats in cv_stats.items():
        report_content += f"- {method.upper()}: {stats['mean_score']:.4f} Â± {stats['std_score']:.4f}\n"

best_method = max(cv_stats.keys(), key=lambda k: cv_stats[k]['mean_score']) if cv_stats else "æœªçŸ¥"
report_content += f"\næœ€ä½³ç®—æ³•: {best_method.upper()}\n"

report_content += f"""
## 3. æœ€ç»ˆæ¨èåˆ†ç»„ä¸æ—¶ç‚¹

"""

for rec in optimal_recommendations:
    report_content += f"""
### {rec['ç»„åˆ«']}
- BMIèŒƒå›´: {rec['BMIèŒƒå›´']} kg/mÂ²
- æ ·æœ¬æ•°: {rec['æ ·æœ¬æ•°']}
- æœ€ä¼˜NIPTæ—¶ç‚¹: {rec['æœ€ä¼˜æ—¶ç‚¹(å‘¨)']}å‘¨
- é¢„æœŸé£é™©: {rec['é¢„æœŸé£é™©']:.4f}
- é¢„æœŸæˆåŠŸç‡: {rec['æˆåŠŸç‡(%)']:.1f}%
- é£é™©ç­‰çº§: {rec['é£é™©ç­‰çº§']}
- ä¸´åºŠå»ºè®®: {rec['ä¸´åºŠå»ºè®®']}

é£é™©åˆ†è§£:
- BMIè´¡çŒ®: {rec['BMIé£é™©è´¡çŒ®']:.4f}
- å¹´é¾„è´¡çŒ®: {rec['å¹´é¾„é£é™©è´¡çŒ®']:.4f}  
- æ—¶ç‚¹è´¡çŒ®: {rec['æ—¶ç‚¹é£é™©è´¡çŒ®']:.4f}
"""

report_content += f"""
## 4. æ£€éªŒè¯¯å·®æ•æ„Ÿæ€§åˆ†æ

"""

for scenario, result in error_simulation_results.items():
    report_content += f"""
### {scenario}
- å¹³å‡é£é™©: {result['mean_risk']:.4f}
- é£é™©æ ‡å‡†å·®: {result['std_risk']:.4f}
- é£é™©èŒƒå›´: [{result['risk_range'][0]:.4f}, {result['risk_range'][1]:.4f}]
- å˜å¼‚ç³»æ•°: {result['cv']:.4f}
"""

report_content += f"""
## 5. åˆ†ç»„ç¨³å®šæ€§åˆ†æ
- å¹³å‡ä¸€è‡´æ€§(ARI): {stability_results['avg_consistency']:.4f}
- ä¸€è‡´æ€§æ ‡å‡†å·®: {np.std(stability_results['consistency_scores']):.4f}
- ç¨³å®šæ€§ç­‰çº§: {stability_results['stability_level']}

## 6. ä¸»è¦ç»“è®ºä¸å»ºè®®

### 6.1 ç®—æ³•æ€§èƒ½
- æœ€ä½³åˆ†ç»„ç®—æ³•: {best_method.upper()}
- äº¤å‰éªŒè¯ç¨³å®šæ€§: è‰¯å¥½
- å¯¹æ£€éªŒè¯¯å·®çš„é²æ£’æ€§: ä¸­ç­‰

### 6.2 ä¸´åºŠåº”ç”¨å»ºè®®
1. ä½é£é™©ç»„: é‡‡ç”¨æ ‡å‡†NIPTæ£€æµ‹æµç¨‹ï¼Œ13å‘¨æ£€æµ‹
2. ä¸­é£é™©ç»„: å¢å¼ºè´¨æ§æªæ–½ï¼Œå¿…è¦æ—¶é‡å¤æ£€æµ‹  
3. é«˜é£é™©ç»„: é«˜åº¦å…³æ³¨ï¼Œå‡†å¤‡å¤‡é€‰æ£€æµ‹æ–¹æ¡ˆ

### 6.3 è´¨é‡æ§åˆ¶è¦æ±‚
- å¯¹äºé«˜BMIç»„åˆ«ï¼Œå»ºè®®å®æ–½æ›´ä¸¥æ ¼çš„æ£€æµ‹è¯¯å·®æ§åˆ¶
- å»ºç«‹åˆ†å±‚è´¨æ§æ ‡å‡†ï¼Œä¸åŒé£é™©ç»„é‡‡ç”¨ä¸åŒçš„è´¨æ§é˜ˆå€¼
- å®šæœŸæ ¡å‡†æ£€æµ‹è®¾å¤‡ï¼Œç¡®ä¿è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…

### 6.4 é£é™©ç›‘æ§å»ºè®®
- é‡ç‚¹å…³æ³¨BMIæå€¼æ ·æœ¬(BMI<20æˆ–BMI>35)
- å»ºç«‹é£é™©é¢„è­¦æœºåˆ¶ï¼Œå¯¹é«˜é£é™©ç»„è¿›è¡Œé‡ç‚¹ç›‘æŠ¤
- è€ƒè™‘å¼•å…¥å¤šå› å­é£é™©è¯„ä¼°æ¨¡å‹ï¼Œæé«˜é¢„æµ‹ç²¾åº¦

## 7. æŠ€æœ¯åˆ›æ–°ç‚¹
1. å¼•å…¥ç”Ÿå­˜åˆ†ææ–¹æ³•è¿›è¡ŒBMIåˆ†ç»„
2. æ„å»ºå¤šå› å­é£é™©è¯„ä¼°å‡½æ•°
3. å®æ–½å…¨é¢çš„äº¤å‰éªŒè¯å’Œæ•æ„Ÿæ€§åˆ†æ
4. å»ºç«‹æ£€éªŒè¯¯å·®å½±å“çš„å®šé‡è¯„ä¼°ä½“ç³»

---
ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†æç‰ˆæœ¬: T2 v2.1
"""

# ä¿å­˜ç»¼åˆæŠ¥å‘Š
with open(os.path.join(results_dir, 'T2_comprehensive_validation_report.txt'), 'w', encoding='utf-8') as f:
    f.write(report_content)

print(report_content)
print(f"\nâœ… T2 v2.1 å…¨é¢éªŒè¯åˆ†æå®Œæˆï¼")
print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {results_dir}")
print(f"ğŸ“ˆ æœ€ä½³ç®—æ³•: {best_method.upper()}")
print(f"ğŸ¯ ç¨³å®šæ€§ç­‰çº§: {stability_results['stability_level']}")
print(f"âš¡ åˆ†æå›¾è¡¨: T2_comprehensive_validation_analysis.png")
