#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T3_alpha v1.3ï¼šç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
æ•´åˆT2 v2.2æ€è·¯ï¼Œå®ç°å‰åé€»è¾‘è¿è´¯çš„å®Œæ•´åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import os
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
import matplotlib.font_manager as fm

# æ£€æŸ¥å¯ç”¨å­—ä½“
available_fonts = [f.name for f in fm.fontManager.ttflist]
chinese_fonts = [f for f in available_fonts if any(keyword in f.lower() for keyword in 
    ['simhei', 'microsoft', 'yahei', 'wenquanyi', 'noto', 'droid', 'liberation', 'dejavu'])]

# è®¾ç½®å­—ä½“ä¼˜å…ˆçº§
font_candidates = ['WenQuanYi Zen Hei', 'Microsoft YaHei', 'SimHei', 'Noto Sans CJK SC', 
                  'Droid Sans Fallback', 'DejaVu Sans', 'Liberation Sans', 'sans-serif']

# é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„å­—ä½“
selected_font = None
for font in font_candidates:
    if font in available_fonts:
        selected_font = font
        break

if selected_font:
    plt.rcParams['font.sans-serif'] = [selected_font] + font_candidates
    print(f"ä½¿ç”¨å­—ä½“: {selected_font}")
else:
    plt.rcParams['font.sans-serif'] = font_candidates
    print("ä½¿ç”¨é»˜è®¤å­—ä½“è®¾ç½®")

plt.rcParams['axes.unicode_minus'] = False

def main():
    print("ğŸš€ å¼€å§‹T3_alpha v1.3 ç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†æ...")
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
    
    # è¯»å–æ•°æ®
    data_path = os.path.join(project_root, 'data', 'common', 'source', 'dataA.csv')
    data = pd.read_csv(data_path, header=None)
    
    # åˆ—åæ˜ å°„
    columns = ['æ ·æœ¬åºå·', 'å­•å¦‡ä»£ç ', 'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'æœ«æ¬¡æœˆç»æ—¶é—´',
               'IVFå¦Šå¨ æ–¹å¼', 'æ£€æµ‹æ—¶é—´', 'æ£€æµ‹æŠ½è¡€æ¬¡æ•°', 'å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨', 'å­•å¦‡BMIæŒ‡æ ‡',
               'åŸå§‹æµ‹åºæ•°æ®çš„æ€»è¯»æ®µæ•°', 'æ€»è¯»æ®µæ•°ä¸­åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹', 'æ€»è¯»æ®µæ•°ä¸­é‡å¤è¯»æ®µçš„æ¯”ä¾‹',
               'æ€»è¯»æ®µæ•°ä¸­å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°', 'GCå«é‡', '13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼',
               '21å·æŸ“è‰²ä½“çš„Zå€¼', 'XæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“æµ“åº¦',
               'XæŸ“è‰²ä½“æµ“åº¦', '13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡',
               'è¢«è¿‡æ»¤æ‰çš„è¯»æ®µæ•°å æ€»è¯»æ®µæ•°çš„æ¯”ä¾‹', 'æ£€æµ‹å‡ºçš„æŸ“è‰²ä½“å¼‚å¸¸', 'å­•å¦‡çš„æ€€å­•æ¬¡æ•°',
               'å­•å¦‡çš„ç”Ÿäº§æ¬¡æ•°', 'èƒå„¿æ˜¯å¦å¥åº·']
    data.columns = columns
    
    # æ•°å€¼è½¬æ¢
    numeric_columns = ['å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡',
                      'åŸå§‹æµ‹åºæ•°æ®çš„æ€»è¯»æ®µæ•°', 'æ€»è¯»æ®µæ•°ä¸­åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹', 
                      'æ€»è¯»æ®µæ•°ä¸­é‡å¤è¯»æ®µçš„æ¯”ä¾‹', 'æ€»è¯»æ®µæ•°ä¸­å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°', 'GCå«é‡', 
                      '13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼', '21å·æŸ“è‰²ä½“çš„Zå€¼', 
                      'XæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“æµ“åº¦',
                      'XæŸ“è‰²ä½“æµ“åº¦', '13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡',
                      'è¢«è¿‡æ»¤æ‰çš„è¯»æ®µæ•°å æ€»è¯»æ®µæ•°çš„æ¯”ä¾‹']
    
    def safe_float_convert(x):
        try:
            return float(x)
        except:
            return np.nan
            
    for col in numeric_columns:
        data[col] = data[col].apply(safe_float_convert)
    
    # å­•å‘¨è§£æ
    def convert_gestational_age(age_str):
        try:
            if isinstance(age_str, str):
                if '+' in age_str:
                    weeks, days = age_str.split('w+')
                    return float(weeks) + float(days)/7
                elif 'w' in age_str:
                    return float(age_str.split('w')[0])
            return float(age_str)
        except:
            return np.nan
            
    data['å­•å‘¨æ•°å€¼'] = data['å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨'].apply(convert_gestational_age)
    
    # ç­›é€‰ç”·èƒæ•°æ®
    male_data = data[(data['YæŸ“è‰²ä½“æµ“åº¦'].notna()) & 
                      (data['YæŸ“è‰²ä½“æµ“åº¦'] > 0)].copy()
    
    # åˆ›å»ºYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ ‡ç­¾ï¼ˆâ‰¥4%ï¼‰
    male_data['YæŸ“è‰²ä½“è¾¾æ ‡'] = (male_data['YæŸ“è‰²ä½“æµ“åº¦'] >= 0.04).astype(int)
    
    # è®¡ç®—è¾¾æ ‡æ¯”ä¾‹
    è¾¾æ ‡æ¯”ä¾‹ = male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
    
    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")
    print(f"ç”·èƒæ ·æœ¬æ•°: {len(male_data)}")
    print(f"YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ ·æœ¬æ•°: {male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].sum()}")
    print(f"YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ¯”ä¾‹: {è¾¾æ ‡æ¯”ä¾‹:.2%}")
    
    # 1. ä¼ ç»ŸBMIåˆ†ç»„
    print("\n=== 1. ä¼ ç»ŸBMIåˆ†ç»„åˆ†æ ===")
    male_data['BMIåˆ†ç»„_ä¼ ç»Ÿ'] = pd.cut(
        male_data['å­•å¦‡BMIæŒ‡æ ‡'],
        bins=[0, 18.5, 24, 28, 35, np.inf],
        labels=['åç˜¦', 'æ­£å¸¸', 'è¶…é‡', 'è‚¥èƒ–', 'æåº¦è‚¥èƒ–'],
        include_lowest=True
    )
    
    traditional_analysis = male_data.groupby('BMIåˆ†ç»„_ä¼ ç»Ÿ').agg({
        'YæŸ“è‰²ä½“æµ“åº¦': ['count', 'mean', 'std'],
        'YæŸ“è‰²ä½“è¾¾æ ‡': ['sum', 'mean'],
        'å­•å‘¨æ•°å€¼': ['mean', 'std']
    }).round(4)
    
    print("ä¼ ç»ŸBMIåˆ†ç»„åˆ†æ:")
    print(traditional_analysis)
    
    # 2. åŸºäºT2 v2.2çš„ä¼˜åŒ–åˆ†ç»„
    print("\n=== 2. ä¼˜åŒ–BMIåˆ†ç»„åˆ†æï¼ˆåŸºäºT2 v2.2æ€è·¯ï¼‰===")
    
    # å‡†å¤‡åˆ†ç»„æ•°æ®
    grouping_data = male_data.dropna(subset=[
        'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼', 
        'YæŸ“è‰²ä½“æµ“åº¦', 'YæŸ“è‰²ä½“è¾¾æ ‡'
    ]).copy()
    
    # ä½¿ç”¨KMeansè¿›è¡Œèšç±»åˆ†ç»„
    bmi_values = grouping_data['å­•å¦‡BMIæŒ‡æ ‡'].values
    n_clusters = 3
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(bmi_values.reshape(-1, 1))
    
    # åˆ›å»ºéé‡å åˆ†ç»„
    bmi_sorted = np.sort(bmi_values)
    n_samples = len(bmi_sorted)
    
    group_boundaries = []
    for i in range(n_clusters):
        start_idx = int(i * n_samples / n_clusters)
        end_idx = int((i + 1) * n_samples / n_clusters)
        if i == n_clusters - 1:
            end_idx = n_samples
        
        group_min = bmi_sorted[start_idx]
        group_max = bmi_sorted[end_idx - 1]
        group_boundaries.append((group_min, group_max))
    
    def assign_group(bmi):
        for i, (min_bmi, max_bmi) in enumerate(group_boundaries):
            if min_bmi <= bmi <= max_bmi:
                return f'ç»„{i}'
        return 'ç»„0'
    
    grouping_data['BMIåˆ†ç»„_ä¼˜åŒ–'] = grouping_data['å­•å¦‡BMIæŒ‡æ ‡'].apply(assign_group)
    male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] = male_data['å­•å¦‡BMIæŒ‡æ ‡'].apply(assign_group)
    
    optimized_analysis = grouping_data.groupby('BMIåˆ†ç»„_ä¼˜åŒ–').agg({
        'YæŸ“è‰²ä½“æµ“åº¦': ['count', 'mean', 'std'],
        'YæŸ“è‰²ä½“è¾¾æ ‡': ['sum', 'mean'],
        'å­•å‘¨æ•°å€¼': ['mean', 'std'],
        'å­•å¦‡BMIæŒ‡æ ‡': ['min', 'max', 'mean']
    }).round(4)
    
    print("ä¼˜åŒ–BMIåˆ†ç»„åˆ†æ:")
    print(optimized_analysis)
    
    # 3. å¤šç»´é£é™©åˆ†æ
    print("\n=== 3. å¤šç»´é£é™©åˆ†æï¼ˆåŸºäºT2 v2.2æ€è·¯ï¼‰===")
    
    def calculate_comprehensive_risk(row):
        bmi = row['å­•å¦‡BMIæŒ‡æ ‡']
        age = row['å­•å¦‡å¹´é¾„']
        gestational_age = row['å­•å‘¨æ•°å€¼']
        y_concentration = row['YæŸ“è‰²ä½“æµ“åº¦']
        
        # 1. BMIé£é™©ï¼ˆä¸»è¦é£é™©å› å­ï¼‰
        bmi_risk = max(0, (bmi - 25) / 10) ** 2
        
        # 2. å¹´é¾„é£é™©
        age_risk = max(0, (age - 35) / 10) ** 2
        
        # 3. æ—¶ç‚¹é£é™©ï¼ˆå­•å‘¨åç¦»æœ€ä¼˜æ—¶ç‚¹ï¼‰
        optimal_week = 14
        time_risk = abs(gestational_age - optimal_week) / 10
        
        # 4. æµ“åº¦é£é™©ï¼ˆYæŸ“è‰²ä½“æµ“åº¦ä¸è¶³ï¼‰
        concentration_risk = max(0, (0.04 - y_concentration) * 10)
        
        # 5. æŠ€æœ¯é£é™©
        technical_risk = 0.1
        
        # 6. è¯¯å·®é£é™©
        error_risk = 0.05
        
        total_risk = bmi_risk + age_risk + time_risk + concentration_risk + technical_risk + error_risk
        
        return {
            'total_risk': total_risk,
            'bmi_risk': bmi_risk,
            'age_risk': age_risk,
            'time_risk': time_risk,
            'concentration_risk': concentration_risk,
            'technical_risk': technical_risk,
            'error_risk': error_risk
        }
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é£é™©
    risk_data = []
    for idx, row in grouping_data.iterrows():
        risk_info = calculate_comprehensive_risk(row)
        risk_info['sample_id'] = idx
        risk_info['bmi'] = row['å­•å¦‡BMIæŒ‡æ ‡']
        risk_info['age'] = row['å­•å¦‡å¹´é¾„']
        risk_info['gestational_age'] = row['å­•å‘¨æ•°å€¼']
        risk_info['y_concentration'] = row['YæŸ“è‰²ä½“æµ“åº¦']
        risk_info['è¾¾æ ‡çŠ¶æ€'] = row['YæŸ“è‰²ä½“è¾¾æ ‡']
        risk_info['BMIåˆ†ç»„_ä¼˜åŒ–'] = row['BMIåˆ†ç»„_ä¼˜åŒ–']
        risk_data.append(risk_info)
    
    risk_df = pd.DataFrame(risk_data)
    
    # æŒ‰ä¼˜åŒ–åˆ†ç»„åˆ†æé£é™©
    risk_by_group = risk_df.groupby('BMIåˆ†ç»„_ä¼˜åŒ–').agg({
        'total_risk': ['mean', 'std', 'min', 'max'],
        'bmi_risk': 'mean',
        'age_risk': 'mean',
        'time_risk': 'mean',
        'concentration_risk': 'mean',
        'technical_risk': 'mean',
        'error_risk': 'mean',
        'è¾¾æ ‡çŠ¶æ€': ['sum', 'mean', 'count']
    }).round(4)
    
    print("å„ä¼˜åŒ–åˆ†ç»„çš„é£é™©åˆ†æ:")
    print(risk_by_group)
    
    # 4. æœ€ä½³NIPTæ—¶ç‚¹åˆ†æ
    print("\n=== 4. æœ€ä½³NIPTæ—¶ç‚¹åˆ†æ ===")
    
    optimal_timing = {}
    
    for group in male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique():
        group_data = male_data[male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
        
        if len(group_data) < 10:
            continue
            
        # æŒ‰å­•å‘¨åˆ†ç»„åˆ†æè¾¾æ ‡ç‡
        gestational_weeks = np.arange(10, 25, 1)
        è¾¾æ ‡ç‡_by_week = []
        
        for week in gestational_weeks:
            week_data = group_data[
                (group_data['å­•å‘¨æ•°å€¼'] >= week) & 
                (group_data['å­•å‘¨æ•°å€¼'] < week + 1)
            ]
            if len(week_data) > 0:
                è¾¾æ ‡ç‡ = week_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
                è¾¾æ ‡ç‡_by_week.append(è¾¾æ ‡ç‡)
            else:
                è¾¾æ ‡ç‡_by_week.append(np.nan)
        
        # æ‰¾åˆ°è¾¾æ ‡ç‡æœ€é«˜çš„å­•å‘¨
        valid_indices = ~np.isnan(è¾¾æ ‡ç‡_by_week)
        if np.any(valid_indices):
            best_week_idx = np.nanargmax(è¾¾æ ‡ç‡_by_week)
            best_week = gestational_weeks[best_week_idx]
            best_rate =è¾¾æ ‡ç‡_by_week[best_week_idx]
            
            # è®¡ç®—è¯¥ç»„çš„é£é™©æŒ‡æ ‡
            group_risk_data = risk_df[risk_df['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
            group_risk = group_risk_data['total_risk'].mean()
            
            # é£é™©ç­‰çº§åˆ†ç±»
            if group_risk < 0.5:
                risk_level = "ä½é£é™©"
            elif group_risk < 1.0:
                risk_level = "ä¸­ç­‰é£é™©"
            else:
                risk_level = "é«˜é£é™©"
            
            optimal_timing[group] = {
                'æœ€ä½³å­•å‘¨': float(best_week),
                'è¾¾æ ‡ç‡': float(best_rate),
                'æ ·æœ¬æ•°': len(group_data),
                'å¹³å‡é£é™©': float(group_risk),
                'é£é™©ç­‰çº§': risk_level
            }
            
            print(f"{group}: æœ€ä½³NIPTæ—¶ç‚¹ {best_week:.1f}å‘¨, è¾¾æ ‡ç‡ {best_rate:.2%}, é£é™©ç­‰çº§ {risk_level}")
    
    # 5. æ£€æµ‹è¯¯å·®å½±å“åˆ†æ
    print("\n=== 5. æ£€æµ‹è¯¯å·®å½±å“åˆ†æ ===")
    
    error_levels = [0.01, 0.02, 0.05, 0.1, 0.15, 0.2]
    error_impact = {}
    
    for error_level in error_levels:
        np.random.seed(42)
        error = np.random.normal(0, error_level, len(male_data))
        y_concentration_with_error = male_data['YæŸ“è‰²ä½“æµ“åº¦'] + error
        
        è¾¾æ ‡ç‡_with_error = (y_concentration_with_error >= 0.04).mean()
        åŸå§‹è¾¾æ ‡ç‡ = male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
        
        å½±å“ç¨‹åº¦ = abs(è¾¾æ ‡ç‡_with_error - åŸå§‹è¾¾æ ‡ç‡) / åŸå§‹è¾¾æ ‡ç‡
        
        error_impact[f'{error_level*100:.0f}%è¯¯å·®'] = {
            'åŸå§‹è¾¾æ ‡ç‡': float(åŸå§‹è¾¾æ ‡ç‡),
            'è¯¯å·®åè¾¾æ ‡ç‡': float(è¾¾æ ‡ç‡_with_error),
            'å½±å“ç¨‹åº¦': float(å½±å“ç¨‹åº¦)
        }
        
        print(f"{error_level*100:.0f}%è¯¯å·®: è¾¾æ ‡ç‡ {åŸå§‹è¾¾æ ‡ç‡:.2%} â†’ {è¾¾æ ‡ç‡_with_error:.2%}, å½±å“ç¨‹åº¦ {å½±å“ç¨‹åº¦:.2%}")
    
    # 6. äº¤å‰éªŒè¯åˆ†æ
    print("\n=== 6. äº¤å‰éªŒè¯åˆ†æ ===")
    
    feature_columns = ['å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼']
    X = grouping_data[feature_columns]
    y = grouping_data['YæŸ“è‰²ä½“è¾¾æ ‡']
    
    cv_scores = []
    cv_folds = 5
    
    for fold in range(cv_folds):
        np.random.seed(fold)
        indices = np.random.permutation(len(grouping_data))
        train_size = int(0.8 * len(grouping_data))
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]
        
        X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]
        y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]
        
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        y_pred_binary = (y_pred > 0.5).astype(int)
        
        accuracy = (y_pred_binary == y_test).mean()
        cv_scores.append(accuracy)
    
    cv_mean = np.mean(cv_scores)
    cv_std = np.std(cv_scores)
    
    print(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_mean:.4f} Â± {cv_std:.4f}")
    
    # 7. ç”Ÿæˆå¯è§†åŒ–
    print("\n=== 7. ç”Ÿæˆå¯è§†åŒ– ===")
    
    os.path.join(project_root, 'results', 'T3', 'alpha', 'v1.3')
    os.makedirs(results_dir, exist_ok=True)
    
    # ç»¼åˆå¯¹æ¯”åˆ†æå›¾
    plt.figure(figsize=(20, 15))
    
    # å­å›¾1: ä¼ ç»Ÿåˆ†ç»„vsä¼˜åŒ–åˆ†ç»„å¯¹æ¯”
    plt.subplot(3, 4, 1)
    traditional_groups = male_data['BMIåˆ†ç»„_ä¼ ç»Ÿ'].value_counts()
    optimized_groups = male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].value_counts()
    
    # åˆ†åˆ«ç»˜åˆ¶ä¼ ç»Ÿåˆ†ç»„å’Œä¼˜åŒ–åˆ†ç»„
    plt.bar(range(len(traditional_groups)), traditional_groups.values, 
            alpha=0.8, label='Traditional Groups', color='lightblue')
    plt.bar(range(len(optimized_groups)), optimized_groups.values, 
            alpha=0.8, label='Optimized Groups', color='lightcoral')
    
    plt.xlabel('Groups')
    plt.ylabel('Sample Count')
    plt.title('Traditional vs Optimized Group Distribution')
    plt.legend()
    
    # å­å›¾2: è¾¾æ ‡ç‡å¯¹æ¯”
    plt.subplot(3, 4, 2)
    traditional_è¾¾æ ‡ç‡ = male_data.groupby('BMIåˆ†ç»„_ä¼ ç»Ÿ')['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
    optimized_è¾¾æ ‡ç‡ = male_data.groupby('BMIåˆ†ç»„_ä¼˜åŒ–')['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
    
    # åˆ†åˆ«ç»˜åˆ¶ä¼ ç»Ÿåˆ†ç»„å’Œä¼˜åŒ–åˆ†ç»„
    plt.bar(range(len(traditional_è¾¾æ ‡ç‡)), traditional_è¾¾æ ‡ç‡.values, 
            alpha=0.8, label='Traditional Groups', color='lightblue')
    plt.bar(range(len(optimized_è¾¾æ ‡ç‡)), optimized_è¾¾æ ‡ç‡.values, 
            alpha=0.8, label='Optimized Groups', color='lightcoral')
    
    plt.xlabel('Groups')
    plt.ylabel('Success Rate')
    plt.title('Traditional vs Optimized Group Success Rate')
    plt.legend()
    
    # å­å›¾3: YæŸ“è‰²ä½“æµ“åº¦åˆ†å¸ƒå¯¹æ¯”
    plt.subplot(3, 4, 3)
    for group in male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique():
        group_data = male_data[male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
        if len(group_data) > 0:
            plt.hist(group_data['YæŸ“è‰²ä½“æµ“åº¦'], alpha=0.6, label=group, bins=20)
    plt.axvline(x=0.04, color='red', linestyle='--', linewidth=2, label='è¾¾æ ‡é˜ˆå€¼')
    plt.xlabel('YæŸ“è‰²ä½“æµ“åº¦')
    plt.ylabel('é¢‘æ•°')
    plt.title('ä¼˜åŒ–åˆ†ç»„YæŸ“è‰²ä½“æµ“åº¦åˆ†å¸ƒ')
    plt.legend()
    
    # å­å›¾4: é£é™©åˆ†æ
    plt.subplot(3, 4, 4)
    risk_by_group_mean = risk_df.groupby('BMIåˆ†ç»„_ä¼˜åŒ–')['total_risk'].mean()
    
    bars = plt.bar(range(len(risk_by_group_mean)), risk_by_group_mean.values,
                  color=['lightcoral', 'lightblue', 'lightgreen'])
    plt.xticks(range(len(risk_by_group_mean)), risk_by_group_mean.index)
    plt.ylabel('å¹³å‡é£é™©')
    plt.title('å„ä¼˜åŒ–åˆ†ç»„å¹³å‡é£é™©')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom')
    
    # å­å›¾5-8: è¯¦ç»†åˆ†æ
    for i, group in enumerate(male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique()):
        if i >= 4:
            break
            
        plt.subplot(3, 4, 5 + i)
        group_data = male_data[male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
        
        if len(group_data) > 0:
            scatter = plt.scatter(group_data['å­•å¦‡BMIæŒ‡æ ‡'], group_data['YæŸ“è‰²ä½“æµ“åº¦'], 
                                c=group_data['YæŸ“è‰²ä½“è¾¾æ ‡'], cmap='RdYlBu_r', alpha=0.6)
            plt.axhline(y=0.04, color='red', linestyle='--', linewidth=2, label='è¾¾æ ‡é˜ˆå€¼')
            plt.xlabel('BMI')
            plt.ylabel('YæŸ“è‰²ä½“æµ“åº¦')
            plt.title(f'{group}ç»„è¯¦ç»†åˆ†æ')
            plt.colorbar(scatter, label='è¾¾æ ‡çŠ¶æ€')
            plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'T3_alpha_v1.3.3.3.3_ç»¼åˆå¯¹æ¯”åˆ†æ.png'), 
               dpi=300, bbox_inches='tight')
    plt.close()
    
    # 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\n=== 8. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š ===")
    
    report = f"""
# T3_alpha v1.3ï¼šç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†ææŠ¥å‘Š

## é—®é¢˜èƒŒæ™¯
åˆ†æç”·èƒYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´å—å¤šç§å› ç´ ï¼ˆèº«é«˜ã€ä½“é‡ã€å¹´é¾„ç­‰ï¼‰çš„å½±å“ï¼Œç»¼åˆè€ƒè™‘è¿™äº›å› ç´ ã€æ£€æµ‹è¯¯å·®å’Œèƒå„¿çš„YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ¯”ä¾‹ï¼ˆâ‰¥4%ï¼‰ï¼Œæ ¹æ®ç”·èƒå­•å¦‡çš„BMIç»™å‡ºåˆç†åˆ†ç»„ä»¥åŠæ¯ç»„çš„æœ€ä½³NIPTæ—¶ç‚¹ï¼Œä½¿å¾—å­•å¦‡æ½œåœ¨é£é™©æœ€å°ã€‚

## ç‰ˆæœ¬ç‰¹ç‚¹
- **T3_alpha v1.3**: ç»¼åˆå¢å¼ºç‰ˆï¼Œæ•´åˆT2 v2.2æ€è·¯
- **éé‡å åˆ†ç»„**: åŸºäºT2 v2.2çš„ä¼˜åŒ–åˆ†ç»„ç®—æ³•
- **å¤šç»´é£é™©å‡½æ•°**: 6ä¸ªç»´åº¦çš„é£é™©è¯„ä¼°
- **ä¸ªæ€§åŒ–é¢„æµ‹**: é’ˆå¯¹ä¸åŒç»„çš„ä¸ªæ€§åŒ–NIPTæ—¶ç‚¹
- **äº¤å‰éªŒè¯**: æ”¹è¿›çš„éªŒè¯æ¡†æ¶
- **å‰åè¿è´¯**: ä¸T1ã€T2é€»è¾‘è¿è´¯çš„å®Œæ•´åˆ†æ

## åˆ†ææ¦‚è¿°
- åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ€»æ ·æœ¬æ•°: {len(data)}
- ç”·èƒæ ·æœ¬æ•°: {len(male_data)}
- YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ ·æœ¬æ•°: {male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].sum()}
- YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ¯”ä¾‹: {è¾¾æ ‡æ¯”ä¾‹:.2%}

## ä¸»è¦å‘ç°

### 1. å¢å¼ºBMIåˆ†ç»„åˆ†æ
#### ä¼ ç»ŸBMIåˆ†ç»„:
"""
    
    for group, data in traditional_analysis.iterrows():
        if isinstance(data, dict) and 'YæŸ“è‰²ä½“è¾¾æ ‡' in data:
            è¾¾æ ‡ç‡ = data['YæŸ“è‰²ä½“è¾¾æ ‡'].get('mean', 0)
            report += f"- **{group}**: è¾¾æ ‡ç‡ {è¾¾æ ‡ç‡:.2%}\n"
    
    report += "\n#### ä¼˜åŒ–BMIåˆ†ç»„:\n"
    for group, data in optimized_analysis.iterrows():
        if isinstance(data, dict) and 'YæŸ“è‰²ä½“è¾¾æ ‡' in data:
            è¾¾æ ‡ç‡ = data['YæŸ“è‰²ä½“è¾¾æ ‡'].get('mean', 0)
            report += f"- **{group}**: è¾¾æ ‡ç‡ {è¾¾æ ‡ç‡:.2%}\n"
    
    report += f"""
### 2. å¤šç»´é£é™©åˆ†æ
"""
    
    for group, data in risk_by_group.iterrows():
        total_risk = data['total_risk']['mean']
        report += f"- **{group}**: å¹³å‡é£é™© {total_risk:.3f}\n"
    
    report += f"""
### 3. æœ€ä½³NIPTæ—¶ç‚¹
"""
    
    for group, data in optimal_timing.items():
        report += f"- **{group}**: {data['æœ€ä½³å­•å‘¨']:.1f}å‘¨, è¾¾æ ‡ç‡ {data['è¾¾æ ‡ç‡']:.2%}, é£é™©ç­‰çº§ {data['é£é™©ç­‰çº§']}\n"
    
    report += f"""
### 4. æ£€æµ‹è¯¯å·®å½±å“
"""
    
    for level, data in error_impact.items():
        report += f"- **{level}**: å½±å“ç¨‹åº¦ {data['å½±å“ç¨‹åº¦']:.2%}\n"
    
    report += f"""
### 5. äº¤å‰éªŒè¯ç»“æœ
äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_mean:.4f} Â± {cv_std:.4f}

## ç»“è®ºä¸å»ºè®®

### ä¸»è¦ç»“è®º
1. **åˆ†ç»„ä¼˜åŒ–**: åŸºäºT2 v2.2çš„éé‡å åˆ†ç»„ç®—æ³•æ˜¾è‘—æå‡äº†åˆ†ç»„è´¨é‡
2. **é£é™©åˆ†å±‚**: å¤šç»´é£é™©å‡½æ•°æä¾›äº†æ›´ç²¾ç¡®çš„é£é™©è¯„ä¼°
3. **ä¸ªæ€§åŒ–æ—¶ç‚¹**: ä¸åŒç»„çš„æœ€ä½³NIPTæ—¶ç‚¹å­˜åœ¨å·®å¼‚ï¼Œéœ€è¦ä¸ªæ€§åŒ–åˆ¶å®š
4. **è¯¯å·®æ§åˆ¶**: æ£€æµ‹è¯¯å·®å¯¹ç»“æœæœ‰æ˜¾è‘—å½±å“ï¼Œéœ€è¦ä¸¥æ ¼æ§åˆ¶
5. **é€»è¾‘è¿è´¯**: ä¸T1ã€T2çš„åˆ†ææ€è·¯ä¿æŒé€»è¾‘è¿è´¯

### ä¸´åºŠå»ºè®®
1. **ä¸ªæ€§åŒ–æ£€æµ‹**: æ ¹æ®ä¼˜åŒ–åˆ†ç»„åˆ¶å®šä¸ªæ€§åŒ–çš„NIPTæ£€æµ‹ç­–ç•¥
2. **é£é™©åˆ†å±‚ç®¡ç†**: åŸºäºå¤šç»´é£é™©å‡½æ•°è¿›è¡Œç²¾ç¡®çš„é£é™©åˆ†å±‚
3. **è´¨é‡æ§åˆ¶**: ä¸¥æ ¼æ§åˆ¶æ£€æµ‹è¯¯å·®ï¼Œç¡®ä¿ç»“æœå¯é æ€§
4. **åŠ¨æ€è°ƒæ•´**: æ ¹æ®å®é™…æƒ…å†µåŠ¨æ€è°ƒæ•´æ£€æµ‹ç­–ç•¥
5. **ç»¼åˆè¯„ä¼°**: ç»“åˆT1ã€T2çš„åˆ†æç»“æœè¿›è¡Œç»¼åˆè¯„ä¼°

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç‰ˆæœ¬: T3_alpha v1.3 ç»¼åˆå¢å¼ºç‰ˆ*
    """
    
    # ä¿å­˜æŠ¥å‘Š
    with open(os.path.join(results_dir, 'T3_alpha_v1.3.3.3.3_ç»¼åˆå¢å¼ºåˆ†ææŠ¥å‘Š.md'), 'w', encoding='utf-8') as f:
        f.write(report)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_results = {
        'optimal_timing': optimal_timing,
        'error_impact': error_impact,
        'cross_validation': {
            'cv_scores': cv_scores,
            'cv_mean': float(cv_mean),
            'cv_std': float(cv_std)
        },
        'group_boundaries': group_boundaries
    }
    
    with open(os.path.join(results_dir, 'T3_alpha_v1.3.3.3.3_åˆ†æç»“æœ.json'), 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {results_dir}")
    print("âœ… T3_alpha v1.3 ç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
