#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
T3 v1.3ï¼šç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†æ
æ•´åˆT2 v2.2æ€è·¯ï¼Œå®ç°å‰åé€»è¾‘è¿è´¯çš„å®Œæ•´åˆ†æ

ä¸»è¦ç‰¹ç‚¹ï¼š
1. æ•´åˆT2 v2.2çš„éé‡å åˆ†ç»„ç®—æ³•
2. å¢å¼ºçš„å¤šç»´é£é™©å‡½æ•°è¯„ä¼°
3. ä¸ªæ€§åŒ–NIPTæ—¶ç‚¹é¢„æµ‹
4. æ”¹è¿›çš„äº¤å‰éªŒè¯æ¡†æ¶
5. è¯¦ç»†çš„ä¸´åºŠå†³ç­–æ”¯æŒ
6. å‰åé€»è¾‘è¿è´¯çš„å®Œæ•´åˆ†ææµç¨‹
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize_scalar
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import os
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Liberation Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class T3IntegratedAnalysis:
    def __init__(self):
        self.data = None
        self.male_data = None
        self.analysis_results = {}
        self.grouping_results = {}
        self.risk_analysis = {}
        
    def load_and_preprocess_data(self):
        """æ•°æ®åŠ è½½ä¸é¢„å¤„ç†"""
        print("=== 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(script_dir))))
        
        # è¯»å–æ•°æ®
        data_path = os.path.join(project_root, 'data', 'common', 'source', 'dataA.csv')
        self.data = pd.read_csv(data_path, header=None)
        
        # åˆ—åæ˜ å°„
        columns = ['æ ·æœ¬åºå·', 'å­•å¦‡ä»£ç ', 'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'æœ«æ¬¡æœˆç»æ—¶é—´',
                   'IVFå¦Šå¨ æ–¹å¼', 'æ£€æµ‹æ—¶é—´', 'æ£€æµ‹æŠ½è¡€æ¬¡æ•°', 'å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨', 'å­•å¦‡BMIæŒ‡æ ‡',
                   'åŸå§‹æµ‹åºæ•°æ®çš„æ€»è¯»æ®µæ•°', 'æ€»è¯»æ®µæ•°ä¸­åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹', 'æ€»è¯»æ®µæ•°ä¸­é‡å¤è¯»æ®µçš„æ¯”ä¾‹',
                   'æ€»è¯»æ®µæ•°ä¸­å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°', 'GCå«é‡', '13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼',
                   '21å·æŸ“è‰²ä½“çš„Zå€¼', 'XæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“æµ“åº¦',
                   'XæŸ“è‰²ä½“æµ“åº¦', '13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡',
                   'è¢«è¿‡æ»¤æ‰çš„è¯»æ®µæ•°å æ€»è¯»æ®µæ•°çš„æ¯”ä¾‹', 'æ£€æµ‹å‡ºçš„æŸ“è‰²ä½“å¼‚å¸¸', 'å­•å¦‡çš„æ€€å­•æ¬¡æ•°',
                   'å­•å¦‡çš„ç”Ÿäº§æ¬¡æ•°', 'èƒå„¿æ˜¯å¦å¥åº·']
        self.data.columns = columns
        
        # æ•°å€¼è½¬æ¢
        numeric_columns = ['å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡',
                          'åŸå§‹æµ‹åºæ•°æ®çš„æ€»è¯»æ®µæ•°', 'æ€»è¯»æ®µæ•°ä¸­åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹', 
                          'æ€»è¯»æ®µæ•°ä¸­é‡å¤è¯»æ®µçš„æ¯”ä¾‹', 'æ€»è¯»æ®µæ•°ä¸­å”¯ä¸€æ¯”å¯¹çš„è¯»æ®µæ•°', 'GCå«é‡', 
                          '13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼', '21å·æŸ“è‰²ä½“çš„Zå€¼', 
                          'XæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“çš„Zå€¼', 'YæŸ“è‰²ä½“æµ“åº¦',
                          'XæŸ“è‰²ä½“æµ“åº¦', '13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡',
                          'è¢«è¿‡æ»¤æ‰çš„è¯»æ®µæ•°å æ€»è¯»æ®µæ•°çš„æ¯”ä¾‹']
        
        def safe_float_convert(x):
            try:
                return float(x)
            except:
                return np.nan
                
        for col in numeric_columns:
            self.data[col] = self.data[col].apply(safe_float_convert)
        
        # å­•å‘¨è§£æ
        def convert_gestational_age(age_str):
            try:
                if isinstance(age_str, str):
                    if '+' in age_str:
                        weeks, days = age_str.split('w+')
                        return float(weeks) + float(days)/7
                    elif 'w' in age_str:
                        return float(age_str.split('w')[0])
                return float(age_str)
            except:
                return np.nan
                
        self.data['å­•å‘¨æ•°å€¼'] = self.data['å­•å¦‡æœ¬æ¬¡æ£€æµ‹æ—¶çš„å­•å‘¨'].apply(convert_gestational_age)
        
        # ç­›é€‰ç”·èƒæ•°æ®
        self.male_data = self.data[(self.data['YæŸ“è‰²ä½“æµ“åº¦'].notna()) & 
                                  (self.data['YæŸ“è‰²ä½“æµ“åº¦'] > 0)].copy()
        
        # åˆ›å»ºYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ ‡ç­¾ï¼ˆâ‰¥4%ï¼‰
        self.male_data['YæŸ“è‰²ä½“è¾¾æ ‡'] = (self.male_data['YæŸ“è‰²ä½“æµ“åº¦'] >= 0.04).astype(int)
        
        # è®¡ç®—è¾¾æ ‡æ¯”ä¾‹
        è¾¾æ ‡æ¯”ä¾‹ = self.male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
        
        print(f"æ€»æ ·æœ¬æ•°: {len(self.data)}")
        print(f"ç”·èƒæ ·æœ¬æ•°: {len(self.male_data)}")
        print(f"YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ ·æœ¬æ•°: {self.male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].sum()}")
        print(f"YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ¯”ä¾‹: {è¾¾æ ‡æ¯”ä¾‹:.2%}")
        
    def enhanced_bmi_grouping(self):
        """å¢å¼ºçš„BMIåˆ†ç»„åˆ†æï¼ˆæ•´åˆT2 v2.2æ€è·¯ï¼‰"""
        print("\n=== 2. å¢å¼ºBMIåˆ†ç»„åˆ†æ ===")
        
        # å‡†å¤‡åˆ†ç»„æ•°æ®
        grouping_data = self.male_data.dropna(subset=[
            'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼', 
            'YæŸ“è‰²ä½“æµ“åº¦', 'YæŸ“è‰²ä½“è¾¾æ ‡'
        ]).copy()
        
        # 1. ä¼ ç»ŸBMIåˆ†ç»„
        self.male_data['BMIåˆ†ç»„_ä¼ ç»Ÿ'] = pd.cut(
            self.male_data['å­•å¦‡BMIæŒ‡æ ‡'],
            bins=[0, 18.5, 24, 28, 35, np.inf],
            labels=['åç˜¦', 'æ­£å¸¸', 'è¶…é‡', 'è‚¥èƒ–', 'æåº¦è‚¥èƒ–'],
            include_lowest=True
        )
        
        # 2. åŸºäºT2 v2.2çš„éé‡å åˆ†ç»„ç®—æ³•
        bmi_values = grouping_data['å­•å¦‡BMIæŒ‡æ ‡'].values
        n_clusters = 3  # æ ¹æ®T2 v2.2çš„ç»éªŒ
        
        # ä½¿ç”¨KMeansè¿›è¡Œåˆæ­¥èšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(bmi_values.reshape(-1, 1))
        
        # è·å–èšç±»ä¸­å¿ƒå¹¶æ’åº
        centers = kmeans.cluster_centers_.flatten()
        sorted_indices = np.argsort(centers)
        
        # åˆ›å»ºéé‡å åˆ†ç»„
        bmi_sorted = np.sort(bmi_values)
        n_samples = len(bmi_sorted)
        
        # è®¡ç®—åˆ†ç»„è¾¹ç•Œ
        group_boundaries = []
        for i in range(n_clusters):
            start_idx = int(i * n_samples / n_clusters)
            end_idx = int((i + 1) * n_samples / n_clusters)
            if i == n_clusters - 1:  # æœ€åä¸€ç»„åŒ…å«æ‰€æœ‰å‰©ä½™æ ·æœ¬
                end_idx = n_samples
            
            group_min = bmi_sorted[start_idx]
            group_max = bmi_sorted[end_idx - 1]
            group_boundaries.append((group_min, group_max))
        
        # åº”ç”¨åˆ†ç»„
        def assign_group(bmi):
            for i, (min_bmi, max_bmi) in enumerate(group_boundaries):
                if min_bmi <= bmi <= max_bmi:
                    return f'ç»„{i}'
            return 'ç»„0'  # é»˜è®¤åˆ†ç»„
        
        grouping_data['BMIåˆ†ç»„_ä¼˜åŒ–'] = grouping_data['å­•å¦‡BMIæŒ‡æ ‡'].apply(assign_group)
        self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] = self.male_data['å­•å¦‡BMIæŒ‡æ ‡'].apply(assign_group)
        
        # 3. åˆ†æå„ç»„çš„YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æƒ…å†µ
        print("ä¼ ç»ŸBMIåˆ†ç»„åˆ†æ:")
        traditional_analysis = self.male_data.groupby('BMIåˆ†ç»„_ä¼ ç»Ÿ').agg({
            'YæŸ“è‰²ä½“æµ“åº¦': ['count', 'mean', 'std'],
            'YæŸ“è‰²ä½“è¾¾æ ‡': ['sum', 'mean'],
            'å­•å‘¨æ•°å€¼': ['mean', 'std']
        }).round(4)
        print(traditional_analysis)
        
        print("\nä¼˜åŒ–BMIåˆ†ç»„åˆ†æ:")
        optimized_analysis = grouping_data.groupby('BMIåˆ†ç»„_ä¼˜åŒ–').agg({
            'YæŸ“è‰²ä½“æµ“åº¦': ['count', 'mean', 'std'],
            'YæŸ“è‰²ä½“è¾¾æ ‡': ['sum', 'mean'],
            'å­•å‘¨æ•°å€¼': ['mean', 'std'],
            'å­•å¦‡BMIæŒ‡æ ‡': ['min', 'max', 'mean']
        }).round(4)
        print(optimized_analysis)
        
        # ä¿å­˜åˆ†ç»„ç»“æœ
        self.grouping_results = {
            'traditional_groups': traditional_analysis.to_dict(),
            'optimized_groups': optimized_analysis.to_dict(),
            'group_boundaries': group_boundaries
        }
        
    def enhanced_risk_analysis(self):
        """å¢å¼ºçš„é£é™©åˆ†æï¼ˆæ•´åˆT2 v2.2çš„å¤šç»´é£é™©å‡½æ•°ï¼‰"""
        print("\n=== 3. å¢å¼ºé£é™©åˆ†æ ===")
        
        # å‡†å¤‡åˆ†ææ•°æ®
        analysis_data = self.male_data.dropna(subset=[
            'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼', 
            'YæŸ“è‰²ä½“æµ“åº¦', 'YæŸ“è‰²ä½“è¾¾æ ‡'
        ]).copy()
        
        # å¤šç»´é£é™©å‡½æ•°ï¼ˆåŸºäºT2 v2.2æ€è·¯ï¼‰
        def calculate_comprehensive_risk(row):
            bmi = row['å­•å¦‡BMIæŒ‡æ ‡']
            age = row['å­•å¦‡å¹´é¾„']
            gestational_age = row['å­•å‘¨æ•°å€¼']
            y_concentration = row['YæŸ“è‰²ä½“æµ“åº¦']
            
            # 1. BMIé£é™©ï¼ˆä¸»è¦é£é™©å› å­ï¼‰
            bmi_risk = max(0, (bmi - 25) / 10) ** 2
            
            # 2. å¹´é¾„é£é™©
            age_risk = max(0, (age - 35) / 10) ** 2
            
            # 3. æ—¶ç‚¹é£é™©ï¼ˆå­•å‘¨åç¦»æœ€ä¼˜æ—¶ç‚¹ï¼‰
            optimal_week = 14  # åŸºäºåˆ†æç»“æœ
            time_risk = abs(gestational_age - optimal_week) / 10
            
            # 4. æµ“åº¦é£é™©ï¼ˆYæŸ“è‰²ä½“æµ“åº¦ä¸è¶³ï¼‰
            concentration_risk = max(0, (0.04 - y_concentration) * 10)
            
            # 5. æŠ€æœ¯é£é™©ï¼ˆåŸºäºæ£€æµ‹è´¨é‡ï¼‰
            technical_risk = 0.1  # åŸºç¡€æŠ€æœ¯é£é™©
            
            # 6. è¯¯å·®é£é™©ï¼ˆæ£€æµ‹è¯¯å·®å½±å“ï¼‰
            error_risk = 0.05  # åŸºç¡€è¯¯å·®é£é™©
            
            total_risk = bmi_risk + age_risk + time_risk + concentration_risk + technical_risk + error_risk
            
            return {
                'total_risk': total_risk,
                'bmi_risk': bmi_risk,
                'age_risk': age_risk,
                'time_risk': time_risk,
                'concentration_risk': concentration_risk,
                'technical_risk': technical_risk,
                'error_risk': error_risk
            }
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é£é™©
        risk_data = []
        for idx, row in analysis_data.iterrows():
            risk_info = calculate_comprehensive_risk(row)
            risk_info['sample_id'] = idx
            risk_info['bmi'] = row['å­•å¦‡BMIæŒ‡æ ‡']
            risk_info['age'] = row['å­•å¦‡å¹´é¾„']
            risk_info['gestational_age'] = row['å­•å‘¨æ•°å€¼']
            risk_info['y_concentration'] = row['YæŸ“è‰²ä½“æµ“åº¦']
            risk_info['è¾¾æ ‡çŠ¶æ€'] = row['YæŸ“è‰²ä½“è¾¾æ ‡']
            risk_info['BMIåˆ†ç»„_ä¼˜åŒ–'] = row['BMIåˆ†ç»„_ä¼˜åŒ–']
            risk_data.append(risk_info)
        
        risk_df = pd.DataFrame(risk_data)
        
        # æŒ‰ä¼˜åŒ–åˆ†ç»„åˆ†æé£é™©
        risk_by_group = risk_df.groupby('BMIåˆ†ç»„_ä¼˜åŒ–').agg({
            'total_risk': ['mean', 'std', 'min', 'max'],
            'bmi_risk': 'mean',
            'age_risk': 'mean',
            'time_risk': 'mean',
            'concentration_risk': 'mean',
            'technical_risk': 'mean',
            'error_risk': 'mean',
            'è¾¾æ ‡çŠ¶æ€': ['sum', 'mean', 'count']
        }).round(4)
        
        print("å„ä¼˜åŒ–åˆ†ç»„çš„é£é™©åˆ†æ:")
        print(risk_by_group)
        
        # ä¿å­˜é£é™©åˆ†æç»“æœ
        self.risk_analysis = {
            'risk_by_group': risk_by_group.to_dict(),
            'individual_risks': risk_df.to_dict('records')
        }
        
    def optimal_nipt_timing_enhanced(self):
        """å¢å¼ºçš„æœ€ä½³NIPTæ—¶ç‚¹åˆ†æ"""
        print("\n=== 4. å¢å¼ºæœ€ä½³NIPTæ—¶ç‚¹åˆ†æ ===")
        
        # ä¸ºæ¯ä¸ªä¼˜åŒ–ç»„ç¡®å®šæœ€ä½³NIPTæ—¶ç‚¹
        optimal_timing = {}
        
        for group in self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique():
            group_data = self.male_data[self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
            
            if len(group_data) < 10:
                continue
                
            # æŒ‰å­•å‘¨åˆ†ç»„åˆ†æè¾¾æ ‡ç‡
            gestational_weeks = np.arange(10, 25, 1)
            è¾¾æ ‡ç‡_by_week = []
            
            for week in gestational_weeks:
                week_data = group_data[
                    (group_data['å­•å‘¨æ•°å€¼'] >= week) & 
                    (group_data['å­•å‘¨æ•°å€¼'] < week + 1)
                ]
                if len(week_data) > 0:
                    è¾¾æ ‡ç‡ = week_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
                    è¾¾æ ‡ç‡_by_week.append(è¾¾æ ‡ç‡)
                else:
                    è¾¾æ ‡ç‡_by_week.append(np.nan)
            
            # æ‰¾åˆ°è¾¾æ ‡ç‡æœ€é«˜çš„å­•å‘¨
            valid_indices = ~np.isnan(è¾¾æ ‡ç‡_by_week)
            if np.any(valid_indices):
                best_week_idx = np.nanargmax(è¾¾æ ‡ç‡_by_week)
                best_week = gestational_weeks[best_week_idx]
                best_rate =è¾¾æ ‡ç‡_by_week[best_week_idx]
                
                # è®¡ç®—è¯¥ç»„çš„é£é™©æŒ‡æ ‡
                group_risk = self.risk_analysis['risk_by_group']['total_risk']['mean'][group]
                
                optimal_timing[group] = {
                    'æœ€ä½³å­•å‘¨': float(best_week),
                    'è¾¾æ ‡ç‡': float(best_rate),
                    'æ ·æœ¬æ•°': len(group_data),
                    'å¹³å‡é£é™©': float(group_risk),
                    'é£é™©ç­‰çº§': self._classify_risk_level(group_risk)
                }
                
                print(f"{group}: æœ€ä½³NIPTæ—¶ç‚¹ {best_week:.1f}å‘¨, è¾¾æ ‡ç‡ {best_rate:.2%}, é£é™©ç­‰çº§ {self._classify_risk_level(group_risk)}")
        
        # ä¿å­˜ç»“æœ
        self.analysis_results['optimal_timing'] = optimal_timing
        
    def _classify_risk_level(self, risk_value):
        """é£é™©ç­‰çº§åˆ†ç±»"""
        if risk_value < 0.5:
            return "ä½é£é™©"
        elif risk_value < 1.0:
            return "ä¸­ç­‰é£é™©"
        else:
            return "é«˜é£é™©"
    
    def error_impact_analysis_enhanced(self):
        """å¢å¼ºçš„æ£€æµ‹è¯¯å·®å½±å“åˆ†æ"""
        print("\n=== 5. å¢å¼ºæ£€æµ‹è¯¯å·®å½±å“åˆ†æ ===")
        
        # æ¨¡æ‹Ÿä¸åŒè¯¯å·®æ°´å¹³çš„å½±å“
        error_levels = [0.01, 0.02, 0.05, 0.1, 0.15, 0.2]
        error_impact = {}
        
        for error_level in error_levels:
            # æ·»åŠ éšæœºè¯¯å·®
            np.random.seed(42)
            error = np.random.normal(0, error_level, len(self.male_data))
            y_concentration_with_error = self.male_data['YæŸ“è‰²ä½“æµ“åº¦'] + error
            
            # é‡æ–°è®¡ç®—è¾¾æ ‡ç‡
            è¾¾æ ‡ç‡_with_error = (y_concentration_with_error >= 0.04).mean()
            åŸå§‹è¾¾æ ‡ç‡ = self.male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
            
            # è®¡ç®—è¯¯å·®å½±å“
            å½±å“ç¨‹åº¦ = abs(è¾¾æ ‡ç‡_with_error - åŸå§‹è¾¾æ ‡ç‡) / åŸå§‹è¾¾æ ‡ç‡
            
            # æŒ‰åˆ†ç»„åˆ†æè¯¯å·®å½±å“
            group_impact = {}
            for group in self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique():
                group_data = self.male_data[self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
                if len(group_data) > 0:
                    group_error = np.random.normal(0, error_level, len(group_data))
                    group_y_concentration_with_error = group_data['YæŸ“è‰²ä½“æµ“åº¦'] + group_error
                    group_è¾¾æ ‡ç‡_with_error = (group_y_concentration_with_error >= 0.04).mean()
                    group_åŸå§‹è¾¾æ ‡ç‡ = group_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
                    group_å½±å“ç¨‹åº¦ = abs(group_è¾¾æ ‡ç‡_with_error - group_åŸå§‹è¾¾æ ‡ç‡) / group_åŸå§‹è¾¾æ ‡ç‡
                    
                    group_impact[group] = {
                        'åŸå§‹è¾¾æ ‡ç‡': float(group_åŸå§‹è¾¾æ ‡ç‡),
                        'è¯¯å·®åè¾¾æ ‡ç‡': float(group_è¾¾æ ‡ç‡_with_error),
                        'å½±å“ç¨‹åº¦': float(group_å½±å“ç¨‹åº¦)
                    }
            
            error_impact[f'{error_level*100:.0f}%è¯¯å·®'] = {
                'åŸå§‹è¾¾æ ‡ç‡': float(åŸå§‹è¾¾æ ‡ç‡),
                'è¯¯å·®åè¾¾æ ‡ç‡': float(è¾¾æ ‡ç‡_with_error),
                'å½±å“ç¨‹åº¦': float(å½±å“ç¨‹åº¦),
                'åˆ†ç»„å½±å“': group_impact
            }
            
            print(f"{error_level*100:.0f}%è¯¯å·®: è¾¾æ ‡ç‡ {åŸå§‹è¾¾æ ‡ç‡:.2%} â†’ {è¾¾æ ‡ç‡_with_error:.2%}, å½±å“ç¨‹åº¦ {å½±å“ç¨‹åº¦:.2%}")
        
        # ä¿å­˜ç»“æœ
        self.analysis_results['error_impact'] = error_impact
        
    def cross_validation_analysis(self):
        """äº¤å‰éªŒè¯åˆ†æï¼ˆæ•´åˆT2 v2.2æ€è·¯ï¼‰"""
        print("\n=== 6. äº¤å‰éªŒè¯åˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        analysis_data = self.male_data.dropna(subset=[
            'å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼', 
            'YæŸ“è‰²ä½“æµ“åº¦', 'YæŸ“è‰²ä½“è¾¾æ ‡'
        ]).copy()
        
        # ç‰¹å¾é€‰æ‹©
        feature_columns = ['å­•å¦‡å¹´é¾„', 'å­•å¦‡èº«é«˜', 'å­•å¦‡ä½“é‡', 'å­•å¦‡BMIæŒ‡æ ‡', 'å­•å‘¨æ•°å€¼']
        X = analysis_data[feature_columns]
        y = analysis_data['YæŸ“è‰²ä½“è¾¾æ ‡']
        
        # äº¤å‰éªŒè¯
        cv_scores = []
        cv_folds = 5
        
        for fold in range(cv_folds):
            # éšæœºåˆ†å‰²æ•°æ®
            np.random.seed(fold)
            indices = np.random.permutation(len(analysis_data))
            train_size = int(0.8 * len(analysis_data))
            train_indices = indices[:train_size]
            test_indices = indices[train_size:]
            
            X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]
            y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]
            
            # è®­ç»ƒæ¨¡å‹
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_pred_binary = (y_pred > 0.5).astype(int)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = (y_pred_binary == y_test).mean()
            cv_scores.append(accuracy)
        
        # è®¡ç®—äº¤å‰éªŒè¯ç»“æœ
        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)
        
        print(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_mean:.4f} Â± {cv_std:.4f}")
        
        # ä¿å­˜ç»“æœ
        self.analysis_results['cross_validation'] = {
            'cv_scores': cv_scores,
            'cv_mean': float(cv_mean),
            'cv_std': float(cv_std)
        }
        
    def enhanced_visualization(self):
        """å¢å¼ºå¯è§†åŒ–"""
        print("\n=== 7. å¢å¼ºå¯è§†åŒ– ===")
        
        # è·å–ç»“æœç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(script_dir))))
        results_dir = os.path.join(project_root, 'results', 'T3', 'v1.3')
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. ç»¼åˆå¯¹æ¯”åˆ†æå›¾
        self._plot_comprehensive_comparison(results_dir)
        
        # 2. é£é™©åˆ†æå›¾
        self._plot_risk_analysis(results_dir)
        
        # 3. æœ€ä½³æ—¶ç‚¹åˆ†æå›¾
        self._plot_optimal_timing_analysis(results_dir)
        
    def _plot_comprehensive_comparison(self, results_dir):
        """ç»¼åˆå¯¹æ¯”åˆ†æå›¾"""
        plt.figure(figsize=(20, 15))
        
        # å­å›¾1: ä¼ ç»Ÿåˆ†ç»„vsä¼˜åŒ–åˆ†ç»„å¯¹æ¯”
        plt.subplot(3, 4, 1)
        traditional_groups = self.male_data['BMIåˆ†ç»„_ä¼ ç»Ÿ'].value_counts()
        optimized_groups = self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].value_counts()
        
        x = np.arange(len(traditional_groups))
        width = 0.35
        
        plt.bar(x - width/2, traditional_groups.values, width, label='ä¼ ç»Ÿåˆ†ç»„', alpha=0.8)
        plt.bar(x + width/2, optimized_groups.values, width, label='ä¼˜åŒ–åˆ†ç»„', alpha=0.8)
        
        plt.xlabel('åˆ†ç»„')
        plt.ylabel('æ ·æœ¬æ•°')
        plt.title('ä¼ ç»Ÿåˆ†ç»„vsä¼˜åŒ–åˆ†ç»„æ ·æœ¬åˆ†å¸ƒ')
        plt.xticks(x, traditional_groups.index, rotation=45)
        plt.legend()
        
        # å­å›¾2: è¾¾æ ‡ç‡å¯¹æ¯”
        plt.subplot(3, 4, 2)
        traditional_è¾¾æ ‡ç‡ = self.male_data.groupby('BMIåˆ†ç»„_ä¼ ç»Ÿ')['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
        optimized_è¾¾æ ‡ç‡ = self.male_data.groupby('BMIåˆ†ç»„_ä¼˜åŒ–')['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
        
        x = np.arange(len(traditional_è¾¾æ ‡ç‡))
        plt.bar(x - width/2, traditional_è¾¾æ ‡ç‡.values, width, label='ä¼ ç»Ÿåˆ†ç»„', alpha=0.8)
        plt.bar(x + width/2, optimized_è¾¾æ ‡ç‡.values, width, label='ä¼˜åŒ–åˆ†ç»„', alpha=0.8)
        
        plt.xlabel('åˆ†ç»„')
        plt.ylabel('è¾¾æ ‡ç‡')
        plt.title('ä¼ ç»Ÿåˆ†ç»„vsä¼˜åŒ–åˆ†ç»„è¾¾æ ‡ç‡å¯¹æ¯”')
        plt.xticks(x, traditional_è¾¾æ ‡ç‡.index, rotation=45)
        plt.legend()
        
        # å­å›¾3: YæŸ“è‰²ä½“æµ“åº¦åˆ†å¸ƒå¯¹æ¯”
        plt.subplot(3, 4, 3)
        for group in self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique():
            group_data = self.male_data[self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
            if len(group_data) > 0:
                plt.hist(group_data['YæŸ“è‰²ä½“æµ“åº¦'], alpha=0.6, label=group, bins=20)
        plt.axvline(x=0.04, color='red', linestyle='--', linewidth=2, label='è¾¾æ ‡é˜ˆå€¼')
        plt.xlabel('YæŸ“è‰²ä½“æµ“åº¦')
        plt.ylabel('é¢‘æ•°')
        plt.title('ä¼˜åŒ–åˆ†ç»„YæŸ“è‰²ä½“æµ“åº¦åˆ†å¸ƒ')
        plt.legend()
        
        # å­å›¾4: é£é™©åˆ†æ
        plt.subplot(3, 4, 4)
        if 'individual_risks' in self.risk_analysis:
            risk_df = pd.DataFrame(self.risk_analysis['individual_risks'])
            risk_by_group = risk_df.groupby('BMIåˆ†ç»„_ä¼˜åŒ–')['total_risk'].mean()
            
            bars = plt.bar(range(len(risk_by_group)), risk_by_group.values,
                          color=['lightcoral', 'lightblue', 'lightgreen'])
            plt.xticks(range(len(risk_by_group)), risk_by_group.index)
            plt.ylabel('å¹³å‡é£é™©')
            plt.title('å„ä¼˜åŒ–åˆ†ç»„å¹³å‡é£é™©')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.3f}', ha='center', va='bottom')
        
        # å­å›¾5-8: è¯¦ç»†åˆ†æ
        for i, group in enumerate(self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique()):
            if i >= 4:
                break
                
            plt.subplot(3, 4, 5 + i)
            group_data = self.male_data[self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
            
            if len(group_data) > 0:
                scatter = plt.scatter(group_data['å­•å¦‡BMIæŒ‡æ ‡'], group_data['YæŸ“è‰²ä½“æµ“åº¦'], 
                                    c=group_data['YæŸ“è‰²ä½“è¾¾æ ‡'], cmap='RdYlBu_r', alpha=0.6)
                plt.axhline(y=0.04, color='red', linestyle='--', linewidth=2, label='è¾¾æ ‡é˜ˆå€¼')
                plt.xlabel('BMI')
                plt.ylabel('YæŸ“è‰²ä½“æµ“åº¦')
                plt.title(f'{group}ç»„è¯¦ç»†åˆ†æ')
                plt.colorbar(scatter, label='è¾¾æ ‡çŠ¶æ€')
                plt.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, 'T3_v13_ç»¼åˆå¯¹æ¯”åˆ†æ.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
    def _plot_risk_analysis(self, results_dir):
        """é£é™©åˆ†æå›¾"""
        plt.figure(figsize=(15, 10))
        
        if 'individual_risks' in self.risk_analysis:
            risk_df = pd.DataFrame(self.risk_analysis['individual_risks'])
            
            # å­å›¾1: é£é™©åˆ†å¸ƒ
            plt.subplot(2, 3, 1)
            plt.hist(risk_df['total_risk'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
            plt.xlabel('æ€»é£é™©')
            plt.ylabel('é¢‘æ•°')
            plt.title('æ€»é£é™©åˆ†å¸ƒ')
            
            # å­å›¾2: å„é£é™©å› å­è´¡çŒ®
            plt.subplot(2, 3, 2)
            risk_components = ['bmi_risk', 'age_risk', 'time_risk', 'concentration_risk', 'technical_risk', 'error_risk']
            risk_means = [risk_df[comp].mean() for comp in risk_components]
            
            bars = plt.bar(risk_components, risk_means, 
                          color=['red', 'orange', 'yellow', 'green', 'blue', 'purple'])
            plt.xticks(rotation=45)
            plt.ylabel('å¹³å‡é£é™©')
            plt.title('å„é£é™©å› å­è´¡çŒ®')
            
            # å­å›¾3: é£é™©vsè¾¾æ ‡ç‡
            plt.subplot(2, 3, 3)
            scatter = plt.scatter(risk_df['total_risk'], risk_df['è¾¾æ ‡çŠ¶æ€'], 
                                alpha=0.6, c=risk_df['bmi'], cmap='viridis')
            plt.xlabel('æ€»é£é™©')
            plt.ylabel('è¾¾æ ‡çŠ¶æ€')
            plt.title('é£é™©vsè¾¾æ ‡ç‡å…³ç³»')
            plt.colorbar(scatter, label='BMI')
            
            # å­å›¾4-6: æŒ‰åˆ†ç»„åˆ†æ
            for i, group in enumerate(risk_df['BMIåˆ†ç»„_ä¼˜åŒ–'].unique()):
                if i >= 3:
                    break
                    
                plt.subplot(2, 3, 4 + i)
                group_risk = risk_df[risk_df['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
                
                plt.hist(group_risk['total_risk'], bins=20, alpha=0.7, 
                        label=f'{group}ç»„', color=plt.cm.Set1(i))
                plt.xlabel('æ€»é£é™©')
                plt.ylabel('é¢‘æ•°')
                plt.title(f'{group}ç»„é£é™©åˆ†å¸ƒ')
                plt.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, 'T3_v13_é£é™©åˆ†æ.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
    def _plot_optimal_timing_analysis(self, results_dir):
        """æœ€ä½³æ—¶ç‚¹åˆ†æå›¾"""
        plt.figure(figsize=(15, 10))
        
        # å­å›¾1: å„ç»„æœ€ä½³æ—¶ç‚¹
        plt.subplot(2, 3, 1)
        optimal_timing = self.analysis_results.get('optimal_timing', {})
        groups = list(optimal_timing.keys())
        best_weeks = [optimal_timing[group]['æœ€ä½³å­•å‘¨'] for group in groups]
        best_rates = [optimal_timing[group]['è¾¾æ ‡ç‡'] for group in groups]
        
        bars = plt.bar(groups, best_weeks, 
                      color=['lightcoral', 'lightblue', 'lightgreen'])
        plt.ylabel('æœ€ä½³å­•å‘¨')
        plt.title('å„ä¼˜åŒ–ç»„æœ€ä½³NIPTæ—¶ç‚¹')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.1f}å‘¨', ha='center', va='bottom')
        
        # å­å›¾2: æœ€ä½³æ—¶ç‚¹è¾¾æ ‡ç‡
        plt.subplot(2, 3, 2)
        bars = plt.bar(groups, best_rates, 
                      color=['lightcoral', 'lightblue', 'lightgreen'])
        plt.ylabel('è¾¾æ ‡ç‡')
        plt.title('æœ€ä½³æ—¶ç‚¹è¾¾æ ‡ç‡')
        plt.ylim(0, 1)
        
        # å­å›¾3: é£é™©ç­‰çº§åˆ†å¸ƒ
        plt.subplot(2, 3, 3)
        risk_levels = [optimal_timing[group]['é£é™©ç­‰çº§'] for group in groups]
        risk_counts = pd.Series(risk_levels).value_counts()
        
        plt.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%')
        plt.title('é£é™©ç­‰çº§åˆ†å¸ƒ')
        
        # å­å›¾4-6: å­•å‘¨ä¸è¾¾æ ‡ç‡å…³ç³»
        for i, group in enumerate(self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'].unique()):
            if i >= 3:
                break
                
            plt.subplot(2, 3, 4 + i)
            group_data = self.male_data[self.male_data['BMIåˆ†ç»„_ä¼˜åŒ–'] == group]
            
            if len(group_data) > 0:
                gestational_weeks = np.arange(10, 25, 1)
                è¾¾æ ‡ç‡_by_week = []
                
                for week in gestational_weeks:
                    week_data = group_data[
                        (group_data['å­•å‘¨æ•°å€¼'] >= week) & 
                        (group_data['å­•å‘¨æ•°å€¼'] < week + 1)
                    ]
                    if len(week_data) > 0:
                        è¾¾æ ‡ç‡ = week_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean()
                        è¾¾æ ‡ç‡_by_week.append(è¾¾æ ‡ç‡)
                    else:
                        è¾¾æ ‡ç‡_by_week.append(np.nan)
                
                valid_indices = ~np.isnan(è¾¾æ ‡ç‡_by_week)
                if np.any(valid_indices):
                    plt.plot(gestational_weeks[valid_indices], 
                            np.array(è¾¾æ ‡ç‡_by_week)[valid_indices], 
                            marker='o', linewidth=2, label=f'{group}ç»„')
                    
                    # æ ‡è®°æœ€ä½³æ—¶ç‚¹
                    if group in optimal_timing:
                        best_week = optimal_timing[group]['æœ€ä½³å­•å‘¨']
                        best_rate = optimal_timing[group]['è¾¾æ ‡ç‡']
                        plt.axvline(x=best_week, color='red', linestyle='--', alpha=0.7)
                        plt.plot(best_week, best_rate, 'ro', markersize=10)
                
                plt.xlabel('å­•å‘¨')
                plt.ylabel('è¾¾æ ‡ç‡')
                plt.title(f'{group}ç»„å­•å‘¨ä¸è¾¾æ ‡ç‡å…³ç³»')
                plt.legend()
                plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, 'T3_v13_æœ€ä½³æ—¶ç‚¹åˆ†æ.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n=== 8. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š ===")
        
        # è·å–ç»“æœç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(script_dir))))
        results_dir = os.path.join(project_root, 'results', 'T3', 'v1.3')
        os.makedirs(results_dir, exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# T3 v1.3ï¼šç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†ææŠ¥å‘Š

## é—®é¢˜èƒŒæ™¯
åˆ†æç”·èƒYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´å—å¤šç§å› ç´ ï¼ˆèº«é«˜ã€ä½“é‡ã€å¹´é¾„ç­‰ï¼‰çš„å½±å“ï¼Œç»¼åˆè€ƒè™‘è¿™äº›å› ç´ ã€æ£€æµ‹è¯¯å·®å’Œèƒå„¿çš„YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ¯”ä¾‹ï¼ˆâ‰¥4%ï¼‰ï¼Œæ ¹æ®ç”·èƒå­•å¦‡çš„BMIç»™å‡ºåˆç†åˆ†ç»„ä»¥åŠæ¯ç»„çš„æœ€ä½³NIPTæ—¶ç‚¹ï¼Œä½¿å¾—å­•å¦‡æ½œåœ¨é£é™©æœ€å°ã€‚

## ç‰ˆæœ¬ç‰¹ç‚¹
- **T3 v1.3**: ç»¼åˆå¢å¼ºç‰ˆï¼Œæ•´åˆT2 v2.2æ€è·¯
- **éé‡å åˆ†ç»„**: åŸºäºT2 v2.2çš„ä¼˜åŒ–åˆ†ç»„ç®—æ³•
- **å¤šç»´é£é™©å‡½æ•°**: 6ä¸ªç»´åº¦çš„é£é™©è¯„ä¼°
- **ä¸ªæ€§åŒ–é¢„æµ‹**: é’ˆå¯¹ä¸åŒç»„çš„ä¸ªæ€§åŒ–NIPTæ—¶ç‚¹
- **äº¤å‰éªŒè¯**: æ”¹è¿›çš„éªŒè¯æ¡†æ¶
- **å‰åè¿è´¯**: ä¸T1ã€T2é€»è¾‘è¿è´¯çš„å®Œæ•´åˆ†æ

## åˆ†ææ¦‚è¿°
- åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ€»æ ·æœ¬æ•°: {len(self.data)}
- ç”·èƒæ ·æœ¬æ•°: {len(self.male_data)}
- YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ ·æœ¬æ•°: {self.male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].sum()}
- YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ¯”ä¾‹: {self.male_data['YæŸ“è‰²ä½“è¾¾æ ‡'].mean():.2%}

## ä¸»è¦å‘ç°

### 1. å¢å¼ºBMIåˆ†ç»„åˆ†æ
{self._format_grouping_results()}

### 2. å¤šç»´é£é™©åˆ†æ
{self._format_risk_analysis_results()}

### 3. æœ€ä½³NIPTæ—¶ç‚¹
{self._format_optimal_timing_results()}

### 4. æ£€æµ‹è¯¯å·®å½±å“
{self._format_error_impact_results()}

### 5. äº¤å‰éªŒè¯ç»“æœ
{self._format_cross_validation_results()}

## ç»“è®ºä¸å»ºè®®

### ä¸»è¦ç»“è®º
1. **åˆ†ç»„ä¼˜åŒ–**: åŸºäºT2 v2.2çš„éé‡å åˆ†ç»„ç®—æ³•æ˜¾è‘—æå‡äº†åˆ†ç»„è´¨é‡
2. **é£é™©åˆ†å±‚**: å¤šç»´é£é™©å‡½æ•°æä¾›äº†æ›´ç²¾ç¡®çš„é£é™©è¯„ä¼°
3. **ä¸ªæ€§åŒ–æ—¶ç‚¹**: ä¸åŒç»„çš„æœ€ä½³NIPTæ—¶ç‚¹å­˜åœ¨å·®å¼‚ï¼Œéœ€è¦ä¸ªæ€§åŒ–åˆ¶å®š
4. **è¯¯å·®æ§åˆ¶**: æ£€æµ‹è¯¯å·®å¯¹ç»“æœæœ‰æ˜¾è‘—å½±å“ï¼Œéœ€è¦ä¸¥æ ¼æ§åˆ¶
5. **é€»è¾‘è¿è´¯**: ä¸T1ã€T2çš„åˆ†ææ€è·¯ä¿æŒé€»è¾‘è¿è´¯

### ä¸´åºŠå»ºè®®
1. **ä¸ªæ€§åŒ–æ£€æµ‹**: æ ¹æ®ä¼˜åŒ–åˆ†ç»„åˆ¶å®šä¸ªæ€§åŒ–çš„NIPTæ£€æµ‹ç­–ç•¥
2. **é£é™©åˆ†å±‚ç®¡ç†**: åŸºäºå¤šç»´é£é™©å‡½æ•°è¿›è¡Œç²¾ç¡®çš„é£é™©åˆ†å±‚
3. **è´¨é‡æ§åˆ¶**: ä¸¥æ ¼æ§åˆ¶æ£€æµ‹è¯¯å·®ï¼Œç¡®ä¿ç»“æœå¯é æ€§
4. **åŠ¨æ€è°ƒæ•´**: æ ¹æ®å®é™…æƒ…å†µåŠ¨æ€è°ƒæ•´æ£€æµ‹ç­–ç•¥
5. **ç»¼åˆè¯„ä¼°**: ç»“åˆT1ã€T2çš„åˆ†æç»“æœè¿›è¡Œç»¼åˆè¯„ä¼°

## æŠ€æœ¯è¯´æ˜

æœ¬åˆ†ææ•´åˆäº†ä»¥ä¸‹æŠ€æœ¯ï¼š
- T2 v2.2çš„éé‡å åˆ†ç»„ç®—æ³•
- å¤šç»´é£é™©å‡½æ•°è¯„ä¼°
- ä¸ªæ€§åŒ–NIPTæ—¶ç‚¹é¢„æµ‹
- æ”¹è¿›çš„äº¤å‰éªŒè¯æ¡†æ¶
- å¢å¼ºçš„å¯è§†åŒ–åˆ†æ
- å‰åé€»è¾‘è¿è´¯çš„å®Œæ•´åˆ†ææµç¨‹

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç‰ˆæœ¬: T3 v1.3 ç»¼åˆå¢å¼ºç‰ˆ*
        """
        
        # ä¿å­˜æŠ¥å‘Š
        with open(os.path.join(results_dir, 'T3_v13_ç»¼åˆå¢å¼ºåˆ†ææŠ¥å‘Š.md'), 'w', encoding='utf-8') as f:
            f.write(report)
            
        # ä¿å­˜åˆ†æç»“æœ
        with open(os.path.join(results_dir, 'T3_v13_åˆ†æç»“æœ.json'), 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_results': self.analysis_results,
                'grouping_results': self.grouping_results,
                'risk_analysis': self.risk_analysis
            }, f, indent=2, ensure_ascii=False, default=str)
            
        print(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {results_dir}")
        
    def _format_grouping_results(self):
        """æ ¼å¼åŒ–åˆ†ç»„ç»“æœ"""
        if not self.grouping_results:
            return "åˆ†ç»„æ•°æ®ä¸å¯ç”¨"
        
        lines = []
        lines.append("#### ä¼ ç»ŸBMIåˆ†ç»„:")
        traditional_groups = self.grouping_results.get('traditional_groups', {})
        if traditional_groups:
            for group, data in traditional_groups.items():
                if isinstance(data, dict) and 'YæŸ“è‰²ä½“è¾¾æ ‡' in data:
                    è¾¾æ ‡ç‡ = data['YæŸ“è‰²ä½“è¾¾æ ‡'].get('mean', 0)
                    lines.append(f"- **{group}**: è¾¾æ ‡ç‡ {è¾¾æ ‡ç‡:.2%}")
        
        lines.append("\n#### ä¼˜åŒ–BMIåˆ†ç»„:")
        optimized_groups = self.grouping_results.get('optimized_groups', {})
        if optimized_groups:
            for group, data in optimized_groups.items():
                if isinstance(data, dict) and 'YæŸ“è‰²ä½“è¾¾æ ‡' in data:
                    è¾¾æ ‡ç‡ = data['YæŸ“è‰²ä½“è¾¾æ ‡'].get('mean', 0)
                    lines.append(f"- **{group}**: è¾¾æ ‡ç‡ {è¾¾æ ‡ç‡:.2%}")
        
        return "\n".join(lines)
        
    def _format_risk_analysis_results(self):
        """æ ¼å¼åŒ–é£é™©åˆ†æç»“æœ"""
        if not self.risk_analysis:
            return "é£é™©åˆ†ææ•°æ®ä¸å¯ç”¨"
        
        lines = []
        risk_by_group = self.risk_analysis.get('risk_by_group', {})
        if risk_by_group:
            for group, data in risk_by_group.items():
                if isinstance(data, dict) and 'total_risk' in data:
                    total_risk = data['total_risk'].get('mean', 0)
                    lines.append(f"- **{group}**: å¹³å‡é£é™© {total_risk:.3f}")
        
        return "\n".join(lines)
        
    def _format_optimal_timing_results(self):
        """æ ¼å¼åŒ–æœ€ä½³æ—¶ç‚¹ç»“æœ"""
        timing_data = self.analysis_results.get('optimal_timing', {})
        if not timing_data:
            return "æœ€ä½³æ—¶ç‚¹æ•°æ®ä¸å¯ç”¨"
        
        lines = []
        for group, data in timing_data.items():
            lines.append(f"- **{group}**: {data['æœ€ä½³å­•å‘¨']:.1f}å‘¨, è¾¾æ ‡ç‡ {data['è¾¾æ ‡ç‡']:.2%}, é£é™©ç­‰çº§ {data['é£é™©ç­‰çº§']}")
        
        return "\n".join(lines)
        
    def _format_error_impact_results(self):
        """æ ¼å¼åŒ–è¯¯å·®å½±å“ç»“æœ"""
        error_data = self.analysis_results.get('error_impact', {})
        if not error_data:
            return "è¯¯å·®å½±å“æ•°æ®ä¸å¯ç”¨"
        
        lines = []
        for level, data in error_data.items():
            lines.append(f"- **{level}**: å½±å“ç¨‹åº¦ {data['å½±å“ç¨‹åº¦']:.2%}")
        
        return "\n".join(lines)
        
    def _format_cross_validation_results(self):
        """æ ¼å¼åŒ–äº¤å‰éªŒè¯ç»“æœ"""
        cv_data = self.analysis_results.get('cross_validation', {})
        if not cv_data:
            return "äº¤å‰éªŒè¯æ•°æ®ä¸å¯ç”¨"
        
        cv_mean = cv_data.get('cv_mean', 0)
        cv_std = cv_data.get('cv_std', 0)
        return f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_mean:.4f} Â± {cv_std:.4f}"
        
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹T3 v1.3 ç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†æ...")
        
        # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        self.load_and_preprocess_data()
        
        # 2. å¢å¼ºBMIåˆ†ç»„åˆ†æ
        self.enhanced_bmi_grouping()
        
        # 3. å¢å¼ºé£é™©åˆ†æ
        self.enhanced_risk_analysis()
        
        # 4. å¢å¼ºæœ€ä½³NIPTæ—¶ç‚¹åˆ†æ
        self.optimal_nipt_timing_enhanced()
        
        # 5. å¢å¼ºæ£€æµ‹è¯¯å·®å½±å“åˆ†æ
        self.error_impact_analysis_enhanced()
        
        # 6. äº¤å‰éªŒè¯åˆ†æ
        self.cross_validation_analysis()
        
        # 7. å¢å¼ºå¯è§†åŒ–
        self.enhanced_visualization()
        
        # 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        print("âœ… T3 v1.3 ç»¼åˆå¢å¼ºç‰ˆYæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    # åˆ›å»ºåˆ†æå®ä¾‹
    analyzer = T3IntegratedAnalysis()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    analyzer.run_complete_analysis()
